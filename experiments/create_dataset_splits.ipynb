{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from magneton.io.internal import ProteinDataset, shard_proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following files contain the FoldSeek sequence-based clusters (AFDB50) and structure-based\n",
    "# clusters (FoldSeek-Cluster), subset to only include SwissProt proteins.\n",
    "data_dir = \"/weka/scratch/weka/kellislab/rcalef/data\"\n",
    "interpro_dir = os.path.join(\n",
    "    data_dir,\n",
    "    \"interpro\",\n",
    "    \"103.0\",\n",
    "    \"swissprot\",\n",
    "    \"sharded_swissprot\",\n",
    "    \"with_ss\",\n",
    ")\n",
    "outdir = os.path.join(interpro_dir, \"dataset_splits\")\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "foldseek_dir = os.path.join(data_dir, \"foldseek_cluster\")\n",
    "seq_cluster_path = os.path.join(foldseek_dir, \"afdb50_clusters_swissprot.tsv\")\n",
    "struct_cluster_path = os.path.join(foldseek_dir, \"foldseek_clusters_swissprot.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rep_id</th>\n",
       "      <th>uniprot_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A1Z5LF99</td>\n",
       "      <td>Q7JYV2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B9LLY0</td>\n",
       "      <td>B9LLY0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B9LLY0</td>\n",
       "      <td>A9WHT8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P07316</td>\n",
       "      <td>A3RLE1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P07316</td>\n",
       "      <td>P04344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       rep_id uniprot_id\n",
       "0  A0A1Z5LF99     Q7JYV2\n",
       "1      B9LLY0     B9LLY0\n",
       "2      B9LLY0     A9WHT8\n",
       "3      P07316     A3RLE1\n",
       "4      P07316     P04344"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `rep_id` gives the UniProt ID of the representative for the cluster, `uniprot_id` gives\n",
    "# the UniProt ID for the actual individual protein. File has an extraneous blank column.\n",
    "seq_cluster_assignments = pd.read_table(seq_cluster_path, names=[\"rep_id\", \"uniprot_id\", \"X\"]).drop(columns=[\"X\"])\n",
    "seq_cluster_assignments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rep_id</th>\n",
       "      <th>uniprot_id</th>\n",
       "      <th>clust_type</th>\n",
       "      <th>tax_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A009J8A1</td>\n",
       "      <td>P0ACU7</td>\n",
       "      <td>seq_clust</td>\n",
       "      <td>83333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0A009J8A1</td>\n",
       "      <td>P0ACU8</td>\n",
       "      <td>seq_clust</td>\n",
       "      <td>199310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0A009J8A1</td>\n",
       "      <td>P0ACU9</td>\n",
       "      <td>seq_clust</td>\n",
       "      <td>623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0A010QBH7</td>\n",
       "      <td>P87145</td>\n",
       "      <td>struct_clust</td>\n",
       "      <td>284812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A0A010R299</td>\n",
       "      <td>Q2U1H5</td>\n",
       "      <td>struct_clust</td>\n",
       "      <td>510516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       rep_id uniprot_id    clust_type  tax_id\n",
       "0  A0A009J8A1     P0ACU7     seq_clust   83333\n",
       "1  A0A009J8A1     P0ACU8     seq_clust  199310\n",
       "2  A0A009J8A1     P0ACU9     seq_clust     623\n",
       "3  A0A010QBH7     P87145  struct_clust  284812\n",
       "4  A0A010R299     Q2U1H5  struct_clust  510516"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `rep_id` gives the UniProt ID of the representative for the cluster, `uniprot_id` gives\n",
    "# the UniProt ID for the actual individual protein.\n",
    "# `clust_type` describes the reason the protein is included in the cluster (i.e. based on sequence or structure),\n",
    "# or whether or not FoldSeek authors excluded it and why.\n",
    "# `tax_id` is just taxonomic ID for organism\n",
    "# See description for file `5-allmembers-repId-entryId-cluFlag-taxId.tsv.gz` here:\n",
    "#  https://afdb-cluster.steineggerlab.workers.dev/\n",
    "\n",
    "struct_cluster_assignments = (\n",
    "    pd.read_table(struct_cluster_path, names=[\"rep_id\", \"uniprot_id\", \"clust_type\", \"tax_id\"])\n",
    "    .assign(\n",
    "        clust_type=lambda x: x.clust_type.map({\n",
    "            1: \"seq_clust\",\n",
    "            2: \"struct_clust\",\n",
    "            3: \"fragments\",\n",
    "            4: \"singleton\",\n",
    "        })\n",
    "    )\n",
    ")\n",
    "struct_cluster_assignments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clust_type\n",
       "seq_clust       439154\n",
       "struct_clust     95602\n",
       "singleton         9477\n",
       "fragments         3552\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct_cluster_assignments.clust_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "530601"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prot_dataset = ProteinDataset(interpro_dir, prefix=\"swissprot.with_ss\")\n",
    "prots = list(prot_dataset)\n",
    "dataset_ids = pd.Series([x.uniprot_id for x in prots])\n",
    "len(dataset_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True    530601\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_ids.isin(seq_cluster_assignments.uniprot_id).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True    530601\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_ids.isin(struct_cluster_assignments.uniprot_id).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clust_type\n",
       "seq_clust       430991\n",
       "struct_clust     92059\n",
       "singleton         4962\n",
       "fragments         2589\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    struct_cluster_assignments\n",
    "    .loc[lambda x: x.uniprot_id.isin(dataset_ids)]\n",
    "    .clust_type.value_counts()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train clusters: 59400, datapoints: 423821 (79.88%) proteins per cluster: 7.14\n",
      "Val clusters: 7425, datapoints: 52623 (9.92%) proteins per cluster: 7.09\n",
      "Test clusters: 7426, datapoints: 54157 (10.21%) proteins per cluster: 7.29\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Get the unique cluster identifiers\n",
    "cluster_ids = struct_cluster_assignments['rep_id'].unique()\n",
    "\n",
    "# Shuffle the cluster identifiers randomly\n",
    "np.random.shuffle(cluster_ids)\n",
    "\n",
    "# Calculate the number of clusters for each split\n",
    "total_clusters = len(cluster_ids)\n",
    "train_clusters_count = int(0.8 * total_clusters)\n",
    "val_clusters_count = int(0.1 * total_clusters)\n",
    "test_clusters_count = total_clusters - train_clusters_count - val_clusters_count\n",
    "\n",
    "# Split the shuffled cluster identifiers into three sets\n",
    "train_clusters = cluster_ids[:train_clusters_count]\n",
    "val_clusters = cluster_ids[train_clusters_count:train_clusters_count+val_clusters_count]\n",
    "test_clusters = cluster_ids[train_clusters_count+val_clusters_count:]\n",
    "\n",
    "# Filter the struct_cluster_assignments dataframe based on the cluster splits\n",
    "train_data = struct_cluster_assignments[struct_cluster_assignments['rep_id'].isin(train_clusters)]\n",
    "val_data = struct_cluster_assignments[struct_cluster_assignments['rep_id'].isin(val_clusters)]\n",
    "test_data = struct_cluster_assignments[struct_cluster_assignments['rep_id'].isin(test_clusters)]\n",
    "\n",
    "# Get the corresponding datapoints for each split\n",
    "train_datapoints = dataset_ids[dataset_ids.isin(train_data['uniprot_id'])]\n",
    "val_datapoints = dataset_ids[dataset_ids.isin(val_data['uniprot_id'])]\n",
    "test_datapoints = dataset_ids[dataset_ids.isin(test_data['uniprot_id'])]\n",
    "\n",
    "# Print total number of clusters and datapoints per split\n",
    "def summary(name, clusters_count, split_datapoints):\n",
    "    print(f\"{name} clusters: {clusters_count}, datapoints: {len(split_datapoints)} ({len(split_datapoints) / len(dataset_ids) * 100:.2f}%) proteins per cluster: {len(split_datapoints) / clusters_count:.2f}\")\n",
    "summary(\"Train\", train_clusters_count, train_datapoints)\n",
    "summary(\"Val\", val_clusters_count, val_datapoints)\n",
    "summary(\"Test\", test_clusters_count, test_datapoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm train, val, and test sets are disjoint\n",
    "assert len(set(train_datapoints) & set(val_datapoints)) == 0\n",
    "assert len(set(train_datapoints) & set(test_datapoints)) == 0\n",
    "\n",
    "# Confirm all datapoints have been assigned to one of the splits\n",
    "assert len(set(train_datapoints)) + len(set(val_datapoints)) + len(set(test_datapoints)) == len(set(dataset_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed file 1, starting file 2\n",
      "completed file 2, starting file 3\n",
      "completed file 3, starting file 4\n",
      "completed file 4, starting file 5\n",
      "completed file 5, starting file 6\n",
      "completed file 6, starting file 7\n",
      "completed file 7, starting file 8\n",
      "completed file 8, starting file 9\n",
      "completed file 9, starting file 10\n",
      "completed file 10, starting file 11\n",
      "completed file 11, starting file 12\n",
      "completed file 12, starting file 13\n",
      "completed file 13, starting file 14\n",
      "completed file 14, starting file 15\n",
      "completed file 15, starting file 16\n",
      "completed file 16, starting file 17\n",
      "completed file 17, starting file 18\n",
      "completed file 18, starting file 19\n",
      "completed file 19, starting file 20\n",
      "completed file 20, starting file 21\n",
      "completed file 21, starting file 22\n",
      "completed file 22, starting file 23\n",
      "completed file 23, starting file 24\n",
      "completed file 24, starting file 25\n",
      "completed file 25, starting file 26\n",
      "completed file 26, starting file 27\n",
      "completed file 27, starting file 28\n",
      "completed file 28, starting file 29\n",
      "completed file 29, starting file 30\n",
      "completed file 30, starting file 31\n",
      "completed file 31, starting file 32\n",
      "completed file 32, starting file 33\n",
      "completed file 33, starting file 34\n",
      "completed file 34, starting file 35\n",
      "completed file 35, starting file 36\n",
      "completed file 36, starting file 37\n",
      "completed file 37, starting file 38\n",
      "completed file 38, starting file 39\n",
      "completed file 39, starting file 40\n",
      "completed file 40, starting file 41\n",
      "completed file 41, starting file 42\n",
      "completed file 42, starting file 43\n",
      "completed file 1, starting file 2\n",
      "completed file 2, starting file 3\n",
      "completed file 3, starting file 4\n",
      "completed file 4, starting file 5\n",
      "completed file 5, starting file 6\n",
      "completed file 1, starting file 2\n",
      "completed file 2, starting file 3\n",
      "completed file 3, starting file 4\n",
      "completed file 4, starting file 5\n",
      "completed file 5, starting file 6\n"
     ]
    }
   ],
   "source": [
    "splits = {\n",
    "    \"train\": train_datapoints,\n",
    "    \"val\": val_datapoints,\n",
    "    \"test\": test_datapoints,\n",
    "}\n",
    "lookups = {k:set(v) for k,v in splits.items()}\n",
    "\n",
    "all_splits = []\n",
    "split_prots = defaultdict(list)\n",
    "\n",
    "for prot in prots:\n",
    "    for split, lookup in lookups.items():\n",
    "        if prot.uniprot_id in lookup:\n",
    "            split_prots[split].append(prot)\n",
    "            break\n",
    "\n",
    "\n",
    "for split_name, datapoints in splits.items():\n",
    "    all_splits.append(\n",
    "        datapoints.rename(\"uniprot_id\").to_frame().assign(split=split_name)\n",
    "    )\n",
    "\n",
    "    split_dir = os.path.join(outdir, f\"{split_name}_sharded\")\n",
    "    os.makedirs(split_dir)\n",
    "    shard_proteins(\n",
    "        split_prots[split_name],\n",
    "        split_dir,\n",
    "        prefix=f\"swissprot.with_ss.{split_name}\",\n",
    "        prots_per_file=10000,\n",
    "    )\n",
    "\n",
    "all_splits = pd.concat(all_splits)\n",
    "all_splits.to_csv(os.path.join(outdir, \"dataset_splits.tsv\"), index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
