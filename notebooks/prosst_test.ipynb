{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "prosst_path = \"/home/rcalef/sandbox/repos/external/ProSST\"\n",
    "sys.path.append(prosst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prosst.structure.get_sst_seq import SSTPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Load Model on cuda ----------\n",
      "MODEL: 5.90M parameters\n",
      "---------- Building Subgraphs ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.16it/s]\n",
      "100%|██████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.88s/it]\n"
     ]
    }
   ],
   "source": [
    "example_pdb = \"/weka/scratch/weka/kellislab/rcalef/data/pdb_alphafolddb/AF-A1RZJ9-F1-model_v4.pdb\"\n",
    "predictor = SSTPredictor(structure_vocab_size=2048) # can be 20, 128, 512, 1024, 2048, 4096\n",
    "result = predictor.predict_from_pdb(example_pdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'AF-A1RZJ9-F1-model_v4.pdb',\n",
       " 'aa_seq': 'MRAELPKRVVVERGALQFLPEVLRELGCSKTVVVTDSGVWSVVGSVVEGALRGLAYEVVYIEAADNSNVERARSAARRVEACAVAGLGGGRPVDVAKYAAFMEGLPFVSVPTAISHDGFASPIVALKDPEGNPLSIFTRPPAAVLVDLAVVSRAPRRLLASGVGDIVGKVTSVADARLAQRLTGEEVPEVALRMAETAARMVLDEVDEIASWTERGVGVLAQAGLLAGMAMAVAGSSRPCSGSEHLFSHSLDKYVPWKKSLHGEQVGVGAIIASYLHGFNWRVIRDALAKVGAPTTVEGLGVTGEDAVRALLKARELRKRFTILDVVELNEGLAWKVLRETGVAPTA',\n",
       " '2048_sst_seq': [1534,\n",
       "  1560,\n",
       "  146,\n",
       "  1590,\n",
       "  466,\n",
       "  973,\n",
       "  1423,\n",
       "  246,\n",
       "  314,\n",
       "  1155,\n",
       "  565,\n",
       "  1683,\n",
       "  352,\n",
       "  1348,\n",
       "  336,\n",
       "  196,\n",
       "  699,\n",
       "  913,\n",
       "  2037,\n",
       "  1282,\n",
       "  169,\n",
       "  235,\n",
       "  259,\n",
       "  937,\n",
       "  1304,\n",
       "  235,\n",
       "  388,\n",
       "  1612,\n",
       "  1232,\n",
       "  157,\n",
       "  1751,\n",
       "  1417,\n",
       "  1393,\n",
       "  269,\n",
       "  8,\n",
       "  1336,\n",
       "  646,\n",
       "  397,\n",
       "  83,\n",
       "  397,\n",
       "  64,\n",
       "  967,\n",
       "  1960,\n",
       "  624,\n",
       "  881,\n",
       "  513,\n",
       "  174,\n",
       "  1207,\n",
       "  75,\n",
       "  174,\n",
       "  1380,\n",
       "  1176,\n",
       "  510,\n",
       "  727,\n",
       "  1359,\n",
       "  774,\n",
       "  2002,\n",
       "  774,\n",
       "  1089,\n",
       "  1185,\n",
       "  1838,\n",
       "  505,\n",
       "  372,\n",
       "  1838,\n",
       "  372,\n",
       "  1687,\n",
       "  750,\n",
       "  573,\n",
       "  1339,\n",
       "  1739,\n",
       "  1339,\n",
       "  2037,\n",
       "  1663,\n",
       "  1691,\n",
       "  561,\n",
       "  365,\n",
       "  1938,\n",
       "  2000,\n",
       "  938,\n",
       "  190,\n",
       "  107,\n",
       "  727,\n",
       "  554,\n",
       "  554,\n",
       "  1417,\n",
       "  1089,\n",
       "  662,\n",
       "  78,\n",
       "  78,\n",
       "  541,\n",
       "  1232,\n",
       "  1232,\n",
       "  1232,\n",
       "  1236,\n",
       "  432,\n",
       "  442,\n",
       "  1545,\n",
       "  247,\n",
       "  562,\n",
       "  621,\n",
       "  972,\n",
       "  1328,\n",
       "  1380,\n",
       "  1908,\n",
       "  1279,\n",
       "  107,\n",
       "  107,\n",
       "  1417,\n",
       "  554,\n",
       "  1417,\n",
       "  78,\n",
       "  1851,\n",
       "  541,\n",
       "  489,\n",
       "  116,\n",
       "  661,\n",
       "  661,\n",
       "  1545,\n",
       "  1232,\n",
       "  1612,\n",
       "  773,\n",
       "  228,\n",
       "  126,\n",
       "  1882,\n",
       "  213,\n",
       "  213,\n",
       "  1255,\n",
       "  178,\n",
       "  1558,\n",
       "  598,\n",
       "  1511,\n",
       "  1243,\n",
       "  1034,\n",
       "  1915,\n",
       "  290,\n",
       "  1200,\n",
       "  96,\n",
       "  308,\n",
       "  922,\n",
       "  1204,\n",
       "  785,\n",
       "  246,\n",
       "  1096,\n",
       "  1702,\n",
       "  1244,\n",
       "  78,\n",
       "  196,\n",
       "  1837,\n",
       "  1491,\n",
       "  1533,\n",
       "  1545,\n",
       "  71,\n",
       "  787,\n",
       "  1629,\n",
       "  1571,\n",
       "  116,\n",
       "  787,\n",
       "  116,\n",
       "  987,\n",
       "  987,\n",
       "  661,\n",
       "  987,\n",
       "  383,\n",
       "  383,\n",
       "  383,\n",
       "  1513,\n",
       "  1513,\n",
       "  383,\n",
       "  383,\n",
       "  261,\n",
       "  1102,\n",
       "  1102,\n",
       "  1400,\n",
       "  1400,\n",
       "  277,\n",
       "  1277,\n",
       "  531,\n",
       "  1938,\n",
       "  1206,\n",
       "  388,\n",
       "  1731,\n",
       "  1736,\n",
       "  1736,\n",
       "  1552,\n",
       "  325,\n",
       "  1354,\n",
       "  828,\n",
       "  1692,\n",
       "  1400,\n",
       "  663,\n",
       "  2001,\n",
       "  182,\n",
       "  292,\n",
       "  205,\n",
       "  1572,\n",
       "  477,\n",
       "  995,\n",
       "  1435,\n",
       "  1572,\n",
       "  1520,\n",
       "  1520,\n",
       "  292,\n",
       "  1520,\n",
       "  346,\n",
       "  1520,\n",
       "  1721,\n",
       "  59,\n",
       "  346,\n",
       "  1721,\n",
       "  667,\n",
       "  1566,\n",
       "  1097,\n",
       "  1691,\n",
       "  1339,\n",
       "  1017,\n",
       "  1024,\n",
       "  1140,\n",
       "  27,\n",
       "  72,\n",
       "  1435,\n",
       "  1549,\n",
       "  39,\n",
       "  39,\n",
       "  383,\n",
       "  1135,\n",
       "  1435,\n",
       "  383,\n",
       "  661,\n",
       "  1991,\n",
       "  1005,\n",
       "  1991,\n",
       "  55,\n",
       "  223,\n",
       "  1366,\n",
       "  329,\n",
       "  932,\n",
       "  932,\n",
       "  787,\n",
       "  2001,\n",
       "  1845,\n",
       "  987,\n",
       "  1951,\n",
       "  1102,\n",
       "  408,\n",
       "  408,\n",
       "  960,\n",
       "  925,\n",
       "  2004,\n",
       "  787,\n",
       "  72,\n",
       "  27,\n",
       "  538,\n",
       "  24,\n",
       "  703,\n",
       "  828,\n",
       "  1479,\n",
       "  1176,\n",
       "  184,\n",
       "  1220,\n",
       "  1601,\n",
       "  1991,\n",
       "  661,\n",
       "  987,\n",
       "  987,\n",
       "  987,\n",
       "  1005,\n",
       "  477,\n",
       "  383,\n",
       "  408,\n",
       "  2004,\n",
       "  1435,\n",
       "  39,\n",
       "  408,\n",
       "  72,\n",
       "  72,\n",
       "  1073,\n",
       "  1928,\n",
       "  1325,\n",
       "  412,\n",
       "  608,\n",
       "  412,\n",
       "  1568,\n",
       "  1017,\n",
       "  1572,\n",
       "  1600,\n",
       "  1281,\n",
       "  477,\n",
       "  383,\n",
       "  323,\n",
       "  1721,\n",
       "  1097,\n",
       "  1518,\n",
       "  182,\n",
       "  661,\n",
       "  1077,\n",
       "  17,\n",
       "  1445,\n",
       "  796,\n",
       "  1533,\n",
       "  1157,\n",
       "  1127,\n",
       "  1682,\n",
       "  147,\n",
       "  1425,\n",
       "  277,\n",
       "  1815,\n",
       "  1005,\n",
       "  625,\n",
       "  2004,\n",
       "  925,\n",
       "  39,\n",
       "  1518,\n",
       "  642,\n",
       "  87,\n",
       "  1051,\n",
       "  1577,\n",
       "  2030,\n",
       "  1338,\n",
       "  478,\n",
       "  618,\n",
       "  1682,\n",
       "  337,\n",
       "  1384,\n",
       "  1961,\n",
       "  1582,\n",
       "  16,\n",
       "  1010,\n",
       "  1342,\n",
       "  323,\n",
       "  631,\n",
       "  355,\n",
       "  1639,\n",
       "  631,\n",
       "  706,\n",
       "  54,\n",
       "  355,\n",
       "  1187,\n",
       "  833,\n",
       "  71,\n",
       "  1346,\n",
       "  1005,\n",
       "  353,\n",
       "  353,\n",
       "  1524,\n",
       "  1019,\n",
       "  1337,\n",
       "  1969]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "347"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result[0][\"2048_sst_seq\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "347"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result[0][\"aa_seq\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/rcalef/storage/om_storage/model_weights/ProSST-2048\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_path, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProSSTForMaskedLM(\n",
       "  (prosst): ProSSTModel(\n",
       "    (embeddings): ProSSTEmbeddings(\n",
       "      (word_embeddings): Embedding(25, 768, padding_idx=0)\n",
       "      (ss_embeddings): Embedding(2051, 768)\n",
       "      (ss_layer_norm): ProSSTLayerNorm()\n",
       "      (LayerNorm): ProSSTLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): ProSSTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ProSSTLayer(\n",
       "          (attention): ProSSTAttention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (ss_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (ss_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ProSSTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): ProSSTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ProSSTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ProSSTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): ProSSTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(2048, 768)\n",
       "    )\n",
       "  )\n",
       "  (cls): ProSSTOnlyMLMHead(\n",
       "    (predictions): ProSSTLMPredictionHead(\n",
       "      (transform): ProSSTPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=25, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EsmTokenizer(name_or_path='/home/rcalef/storage/om_storage/model_weights/ProSST-2048', vocab_size=25, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'cls_token': '<cls>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<cls>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"<eos>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t23: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t24: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 1, 13, 17,  3,  6, 12, 15, 11, 17, 20, 20, 20,  6, 17,  8,  3, 12, 16,\n",
       "          7, 12, 15,  6, 20, 12, 17,  6, 12,  8,  4, 18, 11, 19, 20, 20, 20, 19,\n",
       "          5, 18,  8, 20, 21, 18, 20, 20,  8, 18, 20, 20,  6,  8,  3, 12, 17,  8,\n",
       "         12,  3, 22,  6, 20, 20, 22, 10,  6,  3,  3,  5, 14, 18, 14, 20,  6, 17,\n",
       "          3, 17, 18,  3,  3, 17, 17, 20,  6,  3,  4,  3, 20,  3,  8, 12,  8,  8,\n",
       "          8, 17, 15, 20,  5, 20,  3, 11, 22,  3,  3,  7, 13,  6,  8, 12, 15,  7,\n",
       "         20, 18, 20, 15, 19,  3, 10, 18,  9,  5,  8,  7,  3, 18, 15, 10, 20,  3,\n",
       "         12, 11,  5, 15,  6,  8, 14, 15, 12, 18, 10,  7, 19, 17, 15, 15,  3,  3,\n",
       "         20, 12, 20,  5, 12,  3, 20, 20, 18, 17,  3, 15, 17, 17, 12, 12,  3, 18,\n",
       "          8, 20,  8,  5, 10, 20,  8, 11, 20, 19, 18, 20,  3,  5,  3, 17, 12,  3,\n",
       "         16, 17, 12, 19,  8,  6,  6, 20, 15,  6, 20,  3, 12, 17, 13,  3,  6, 19,\n",
       "          3,  3, 17, 13, 20, 12,  5,  6, 20,  5,  6, 10,  3, 18, 21, 19,  6, 17,\n",
       "          8, 20,  8, 20, 12,  3, 16,  3,  8, 12, 12,  3,  8, 13,  3, 13,  3, 20,\n",
       "          3,  8, 18, 18, 17, 15,  4, 18,  8, 18,  6,  9, 12,  7, 18,  9, 18, 12,\n",
       "          5, 11, 22, 20, 15, 21, 11, 11, 18, 12,  9,  8,  6, 16, 20,  8, 20,  8,\n",
       "          3, 10, 10,  3, 18, 22, 12,  9,  8,  7, 14, 21, 17, 20, 10, 17,  5,  3,\n",
       "         12,  3, 11, 20,  8,  3, 15, 19, 19, 20,  6,  8, 12,  8, 20, 19,  8,  6,\n",
       "          5,  3, 20, 17,  3, 12, 12, 11,  3, 17,  6, 12, 17, 11, 17,  7, 19, 10,\n",
       "         12,  5, 20, 20,  6, 12, 14,  6,  8, 12,  3, 21, 11, 20, 12, 17,  6, 19,\n",
       "          8, 20,  3, 15, 19,  3,  2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa_toks = tokenizer(result[0][\"aa_seq\"], return_tensors=\"pt\")\n",
    "aa_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 349])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa_toks[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([347])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss_toks = torch.tensor(result[0][\"2048_sst_seq\"])\n",
    "ss_toks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cat() received an invalid combination of arguments - got (list, Tensor, list), but expected one of:\n * (tuple of Tensors tensors, int dim = 0, *, Tensor out = None)\n * (tuple of Tensors tensors, name dim, *, Tensor out = None)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mss_toks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: cat() received an invalid combination of arguments - got (list, Tensor, list), but expected one of:\n * (tuple of Tensors tensors, int dim = 0, *, Tensor out = None)\n * (tuple of Tensors tensors, name dim, *, Tensor out = None)\n"
     ]
    }
   ],
   "source": [
    "torch.cat([1], ss_toks, [2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 347])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa_toks[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=None, logits=tensor([[[-18.3368, -12.2662, -13.9507,  ...,   0.8657, -15.6547, -18.3795],\n",
       "         [-18.8052, -12.7068, -15.0800,  ...,   0.6976, -16.0266, -18.8201],\n",
       "         [-18.6050, -12.5979, -14.4972,  ...,   0.2606, -15.8135, -18.6339],\n",
       "         ...,\n",
       "         [-18.6463, -12.9135, -14.2035,  ...,   0.7077, -15.9107, -18.6853],\n",
       "         [-18.7029, -13.0057, -14.9421,  ...,   0.2711, -16.3094, -18.7389],\n",
       "         [-19.0306, -13.0453, -15.0684,  ...,   0.3415, -16.1042, -19.0603]]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def forward(\n",
    "#     self,\n",
    "#     input_ids: Optional[torch.Tensor] = None,\n",
    "#     ss_input_ids: Optional[torch.Tensor] = None,\n",
    "#     attention_mask: Optional[torch.Tensor] = None,\n",
    "#     token_type_ids: Optional[torch.Tensor] = None,\n",
    "#     position_ids: Optional[torch.Tensor] = None,\n",
    "#     inputs_embeds: Optional[torch.Tensor] = None,\n",
    "#     labels: Optional[torch.Tensor] = None,\n",
    "#     output_attentions: Optional[bool] = None,\n",
    "#     output_hidden_states: Optional[bool] = None,\n",
    "#     return_dict: Optional[bool] = None,\n",
    "# ) -> Union[Tuple, MaskedLMOutput]:\n",
    "with torch.inference_mode():\n",
    "    out = model(\n",
    "        input_ids=aa_toks[\"input_ids\"],\n",
    "        ss_input_ids=ss_toks,\n",
    "        attention_mask=aa_toks[\"attention_mask\"],\n",
    "    )\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(\"AI4Protein/ProSST-2048\", trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"AI4Protein/ProSST-2048\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/net/vast-storage/scratch/vast/kellislab/rcalef/sandbox/repos/magneton/magneton/external/ProSST\n"
     ]
    }
   ],
   "source": [
    "from magneton.config import (\n",
    "    DataConfig,\n",
    "    EmbeddingConfig,\n",
    "    ModelConfig,\n",
    "    TrainingConfig,\n",
    "    PipelineConfig,\n",
    ")\n",
    "from magneton.embedders.prosst_embedder import ProSSTConfig, ProSSTDataModule, ProSSTEmbedder, move_prosst_inputs_to_device\n",
    "from magneton.embedders.esmc_embedder import ESMCConfig, ESMCDataModule, ESMCEmbedder\n",
    "from magneton.training.embedding_mlp import EmbeddingMLP, MultitaskEmbeddingMLP\n",
    "from magneton.types import InterProType\n",
    "from magneton.utils import move_inputs_to_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpro_path = \"/weka/scratch/weka/kellislab/rcalef/data/interpro/103.0/\"\n",
    "fasta_path = \"/weka/scratch/weka/kellislab/rcalef/data/uniprot/uniprot_sprot.fasta.gz\"\n",
    "labels_path = \"/weka/scratch/weka/kellislab/rcalef/data/interpro/103.0/label_sets/selected_subset\"\n",
    "pickle_path = os.path.join(interpro_path, \"swissprot\", \"sharded_swissprot\", \"with_ss\", \"debug_datasets\")\n",
    "#pickle_path = os.path.join(interpro_path, \"swissprot\", \"sharded_swissprot\", \"with_ss\", \"dataset_splits\", \"seq_splits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/rcalef/storage/om_storage/model_weights/ProSST-2048\"\n",
    "\n",
    "data_config = DataConfig(\n",
    "    data_dir=pickle_path,\n",
    "    prefix=\"swissprot.with_ss\",\n",
    "    fasta_path=fasta_path,\n",
    "    labels_path=labels_path,\n",
    "    substruct_types=[\"Domain\"],\n",
    "    collapse_labels=True,\n",
    "    struct_template=\"/weka/scratch/weka/kellislab/rcalef/data/pdb_alphafolddb/AF-%s-F1-model_v4.pdb\",\n",
    "    model_specific_params={\n",
    "        \"model_path\": model_path,\n",
    "        \"num_threads\": 4,\n",
    "    }\n",
    ")\n",
    "\n",
    "train_config = TrainingConfig(\n",
    "    batch_size=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = ProSSTDataModule(\n",
    "    data_config=data_config,\n",
    "    train_config=train_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3715\n",
      "917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "load = True\n",
    "if load:\n",
    "    data_loader = data_module.train_dataloader()\n",
    "    print(len(data_loader.dataset))\n",
    "    print(data_loader.dataset.substruct_parser.num_labels())\n",
    "    num_labels = data_loader.dataset.substruct_parser.num_labels()\n",
    "else:\n",
    "    num_labels = 917"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1, 13, 13, 11,  7, 11, 15, 14, 16, 19, 17, 19, 22,  5, 17,  6,  8,  7,\n",
       "        11, 11, 17,  3,  3,  4, 12,  4,  7, 17, 18,  6, 16,  6,  5,  6, 20, 12,\n",
       "        12, 20, 18, 18, 18, 17, 22, 15,  5, 16, 21, 10, 20, 15,  8,  8,  8, 13,\n",
       "         6, 15,  6,  6,  6, 15,  8,  8,  3,  3, 20, 17,  6, 20, 22,  6,  6,  3,\n",
       "         8, 20, 11,  8, 11, 12,  8, 17, 12, 12,  8, 10,  7,  6, 16, 14, 16,  5,\n",
       "        17, 11,  9, 17, 19, 22, 20, 22, 20, 12, 19, 20, 19,  6, 10, 12,  6,  5,\n",
       "        21,  6,  5, 18, 20, 14, 10,  8, 17, 11, 17,  6, 21,  7, 11, 20,  6,  5,\n",
       "         3, 10, 11, 20, 12, 16,  4,  9, 11, 15, 20,  9,  3,  6, 22, 12,  6, 11,\n",
       "        12, 11, 12,  8,  4, 18, 15,  3, 14,  8, 14, 18, 19, 20, 15, 18, 12, 15,\n",
       "         5, 14, 14,  3, 12,  7, 20, 19,  3,  3, 16, 19, 18,  8, 12, 15, 18, 18,\n",
       "        20, 17,  2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.dataset[10].sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProSSTBatch(substructures=[[LabeledSubstructure(ranges=[tensor([ 41, 229])], label=767, element_type='Domain')], [LabeledSubstructure(ranges=[tensor([ 10, 103])], label=711, element_type='Domain')]], prot_ids=['A1CCU0', 'A1AE45'], batch_graphs=DataBatch(edge_index=[2, 115420], node_s=[11237, 20], node_v=[11237, 3, 3], edge_s=[115420, 32], edge_v=[115420, 1, 3], batch=[11237], ptr=[688]), tokenized_seqs=tensor([[ 1, 13, 20, 18,  7, 11, 22, 12,  7, 12,  3,  3, 18,  3, 12,  8,  3, 12,\n",
       "          3,  3, 15, 20,  6, 20,  6,  6, 18, 18, 21,  7, 14,  6, 19,  3, 12,  9,\n",
       "          6,  7,  3,  6, 17,  3,  8, 19, 15, 18, 18, 19,  8, 21, 14, 14,  8, 22,\n",
       "         22, 22, 18,  7, 21, 19,  5, 14,  8,  8, 19, 20, 14, 22, 16, 14,  8, 14,\n",
       "          8,  8, 18, 22, 18, 20, 16, 21, 11,  5, 19,  8, 14,  7, 20,  8,  8, 11,\n",
       "          8, 21, 14, 15,  8, 18,  3, 17, 19, 10, 14, 22, 18,  8, 18,  7, 14, 15,\n",
       "         18,  8, 14,  3, 22, 12, 19, 20, 22,  8, 21, 19, 19, 14, 15, 12, 20,  6,\n",
       "         22, 22, 10, 20,  6, 14, 22,  8, 19, 22, 14, 15,  8, 14,  8,  8, 19, 22,\n",
       "         17,  8, 18, 20, 22, 18,  5,  8,  3, 14, 22, 14, 10, 22, 19,  3, 19, 17,\n",
       "         22, 14,  3, 15, 18, 10,  6,  8,  5, 11, 19,  7, 19, 16, 22, 21, 18, 20,\n",
       "         17, 16, 18, 11, 17, 19,  8,  8, 19, 20, 19, 19,  3, 14,  9,  7, 14,  3,\n",
       "         21,  3, 16, 12,  8, 13, 18, 12,  8, 19,  9, 14, 22, 16, 10, 20,  3, 19,\n",
       "          6,  8, 22, 16, 18, 18,  8, 18, 18, 18, 10, 19, 20, 22,  2,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 1, 13, 12, 15, 18, 16, 18, 15,  3, 10,  7, 19, 20, 18, 17, 12, 14, 16,\n",
       "         19, 20, 17, 12, 12, 12,  6,  9,  6, 13,  8, 16, 20, 21, 10, 18,  8,  6,\n",
       "         10, 18, 14,  7, 19, 16, 15,  3, 18,  8,  9, 21, 22,  7, 19, 12, 11,  5,\n",
       "          5, 19,  3, 16, 20, 17,  4,  3, 13,  7, 17, 14, 18, 14, 17, 17, 20, 19,\n",
       "          7, 17, 15, 16,  9,  8, 16, 16, 20, 12, 20, 17,  3, 14, 10, 19, 12, 22,\n",
       "          6, 15, 17,  8,  5, 22, 16, 10, 10, 20,  6, 18, 13, 16, 15,  3,  8,  6,\n",
       "          8, 12, 12, 16, 12, 11, 22,  6, 16, 12, 11,  3, 11, 12, 16,  3,  6,  8,\n",
       "         12,  7,  5, 12, 16, 22, 11, 11, 18, 12, 15, 18, 15,  3,  9,  4, 20,  8,\n",
       "         20, 10, 19, 18, 11, 19,  8,  3,  3, 12,  9,  5, 10, 12,  9, 20, 12, 11,\n",
       "         17, 17,  5, 15, 18, 12, 15, 20, 10, 10, 22, 15, 19,  3, 20, 16,  8,  5,\n",
       "          5,  3, 15,  8, 16, 10, 20, 17,  3, 10,  6, 12,  3, 14, 16, 17, 14,  6,\n",
       "          4,  5, 20, 12, 10, 20,  8, 17,  8,  8,  8, 18, 12,  6,  5, 12, 21, 18,\n",
       "          7, 14,  5,  6, 17, 20,  3, 17,  3, 10,  7,  3, 18, 17, 10, 15, 10, 20,\n",
       "         18,  3, 20,  8,  9,  6, 19,  5, 20, 19, 10,  3,  5,  7, 20,  3,  5, 12,\n",
       "         17,  3, 15, 19, 15, 18,  3,  3,  3,  6, 20, 20, 18, 17, 14, 16, 16,  6,\n",
       "         12, 12, 17, 16, 20, 16, 18, 19,  9, 16, 17, 12,  6, 13,  3, 13,  5, 22,\n",
       "         22, 12,  3, 14, 17, 19, 17, 17,  7, 19, 16, 10,  9,  9, 17, 12, 16, 16,\n",
       "         16,  9, 15, 16, 12, 17, 12,  3, 17, 16, 16, 19, 13, 12,  6, 17, 12, 16,\n",
       "         11, 17, 13, 18,  7,  3, 12,  6, 18, 16, 12, 11, 17,  3,  8, 16, 16, 16,\n",
       "         16, 17, 12, 19, 17, 16, 12, 20, 16, 16, 14, 15, 16, 18, 17, 10,  9, 17,\n",
       "          3, 16, 19, 17, 10, 16, 16, 12,  6, 22, 17, 12,  3,  6, 19, 12, 17,  3,\n",
       "         16, 12, 18,  3, 19, 17,  6, 17,  7,  8, 14,  3, 20, 19,  9, 12,  6,  3,\n",
       "         20, 18, 15, 12, 18, 19, 12,  3, 17,  8, 22, 18, 20, 19, 18,  3,  3,  5,\n",
       "          8,  3, 20, 12, 11, 16, 20, 11, 16, 20, 11, 20,  8,  6, 19, 12, 19, 19,\n",
       "         17, 12,  8,  5,  8, 20, 20, 10, 18,  6, 20, 18,  3, 20, 19, 11, 19, 17,\n",
       "         11, 18, 17, 11, 11, 19, 18, 14, 15,  2]]), protein_lengths=[231, 460])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(data_loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = ModelConfig(\n",
    "    model_type=\"embedding_mlp\",\n",
    "    model_params={\n",
    "        \"hidden_dims\": [256, 256],\n",
    "        \"dropout_rate\": 0.1,\n",
    "    },\n",
    "    frozen_embedder=False,\n",
    ")\n",
    "prosst_config = ProSSTConfig(\n",
    "    model_path=model_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = ProSSTEmbedder(prosst_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = embedder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = batch.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 460])\n",
      "torch.Size([2, 458])\n",
      "torch.Size([2, 460])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (458) must match the size of tensor b (460) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43membedder\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:10\u001b[39m, in \u001b[36membed_batch\u001b[39m\u001b[34m(self, batch)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/net/vast-storage/scratch/vast/kellislab/rcalef/sandbox/repos/magneton/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/net/vast-storage/scratch/vast/kellislab/rcalef/sandbox/repos/magneton/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/om2/user/rcalef/.cache/huggingface/modules/transformers_modules/ProSST-2048/modeling_prosst.py:1187\u001b[39m, in \u001b[36mProSSTForMaskedLM.forward\u001b[39m\u001b[34m(self, input_ids, ss_input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1176\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1177\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[32m   1178\u001b[39m \u001b[33;03m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[32m   1179\u001b[39m \u001b[33;03m    config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\u001b[39;00m\n\u001b[32m   1180\u001b[39m \u001b[33;03m    loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[32m   1181\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1183\u001b[39m return_dict = (\n\u001b[32m   1184\u001b[39m     return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m   1185\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1187\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprosst\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m    \u001b[49m\u001b[43mss_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mss_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1199\u001b[39m sequence_output = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1200\u001b[39m prediction_scores = \u001b[38;5;28mself\u001b[39m.cls(sequence_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/net/vast-storage/scratch/vast/kellislab/rcalef/sandbox/repos/magneton/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/net/vast-storage/scratch/vast/kellislab/rcalef/sandbox/repos/magneton/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/om2/user/rcalef/.cache/huggingface/modules/transformers_modules/ProSST-2048/modeling_prosst.py:1026\u001b[39m, in \u001b[36mProSSTModel.forward\u001b[39m\u001b[34m(self, input_ids, ss_input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m token_type_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1024\u001b[39m     token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m embedding_output, ss_embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1028\u001b[39m \u001b[43m    \u001b[49m\u001b[43mss_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mss_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1033\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1035\u001b[39m encoder_outputs = \u001b[38;5;28mself\u001b[39m.encoder(\n\u001b[32m   1036\u001b[39m     embedding_output,\n\u001b[32m   1037\u001b[39m     attention_mask,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1041\u001b[39m     ss_hidden_states=ss_embeddings,\n\u001b[32m   1042\u001b[39m )\n\u001b[32m   1043\u001b[39m encoded_layers = encoder_outputs[\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/net/vast-storage/scratch/vast/kellislab/rcalef/sandbox/repos/magneton/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/net/vast-storage/scratch/vast/kellislab/rcalef/sandbox/repos/magneton/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/om2/user/rcalef/.cache/huggingface/modules/transformers_modules/ProSST-2048/modeling_prosst.py:920\u001b[39m, in \u001b[36mProSSTEmbeddings.forward\u001b[39m\u001b[34m(self, input_ids, ss_input_ids, token_type_ids, position_ids, mask, inputs_embeds)\u001b[39m\n\u001b[32m    918\u001b[39m         mask = mask.unsqueeze(\u001b[32m2\u001b[39m)\n\u001b[32m    919\u001b[39m     mask = mask.to(ss_embeddings.dtype)\n\u001b[32m--> \u001b[39m\u001b[32m920\u001b[39m     ss_embeddings = \u001b[43mss_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\n\u001b[32m    921\u001b[39m     ss_embeddings = \u001b[38;5;28mself\u001b[39m.dropout(ss_embeddings)\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings, ss_embeddings\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (458) must match the size of tensor b (460) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "embedder.embed_batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.3121, 0.3089, 0.3078, 0.3285, 0.2895, 0.2951, 0.3388, 0.2945, 0.2910,\n",
       "        0.3344, 0.3081, 0.3292, 0.3346, 0.2586, 0.2828, 0.2632, 0.2735, 0.2689,\n",
       "        0.3046, 0.3257], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder.encoder_model.W_v[0].scalar_norm.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Creating esmc embedder ===\n",
      "EmbeddingConfig(_target_='magneton.config.EmbeddingConfig',\n",
      "                model='esmc',\n",
      "                batch_size=32,\n",
      "                model_params={'mask_prob': 0.15,\n",
      "                              'max_seq_length': 2048,\n",
      "                              'model_size': '600m',\n",
      "                              'rep_layer': 33,\n",
      "                              'use_flash_attn': True,\n",
      "                              'weights_path': '/weka/scratch/weka/kellislab/rcalef/model_weights/esmc-600m-2024-12'})\n",
      "Config parameters: None\n"
     ]
    }
   ],
   "source": [
    "pipeline_config = PipelineConfig(\n",
    "    data=data_config,\n",
    "    model=model_config,\n",
    "    training=train_config,\n",
    "    embedding=EmbeddingConfig(\n",
    "        model=\"esmc\",\n",
    "        model_params=esmc_config.__dict__,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Train model\n",
    "if data_config.collapse_labels:\n",
    "    mlp = EmbeddingMLP(\n",
    "        config=pipeline_config,\n",
    "        num_classes=num_labels,\n",
    "    )\n",
    "else:\n",
    "    mlp = MultitaskEmbeddingMLP(\n",
    "        config=pipeline_config,\n",
    "        num_classes=num_labels,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
