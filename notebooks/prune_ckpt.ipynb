{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this file, to prune a specific checkpoint for its MLP layers, replace ckpt and ckpt_path in cell 4 and use \"Run all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[38;5;33mcheckpoints_esmc_300m_active_ewc\u001b[0m/     run.sh              slurm-42279196.out\n",
      "model_esmc_300m_active_ewc_no_mlp.pt  slurm-42279169.out\n",
      "model_esmc_300m_active_ewc.pt         slurm-42279186.out\n"
     ]
    }
   ],
   "source": [
    "ls /net/vast-storage/scratch/vast/kellislab/artliang/magneton/finetuning/esmc_300m_active_ewc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from magneton.config import PipelineConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = '/net/vast-storage/scratch/vast/kellislab/artliang/magneton/finetuning/esmc_300m_homo_ewc/model_esmc_300m_homo_ewc.pt'\n",
    "ckpt_path = '/net/vast-storage/scratch/vast/kellislab/artliang/magneton/finetuning/esmc_300m_homo_ewc/model_esmc_300m_homo_ewc'\n",
    "\n",
    "# special for ss ewc\n",
    "# ckpt = '/net/vast-storage/scratch/vast/kellislab/artliang/magneton/finetuning/esmc_300m_ss_ewc/checkpoints_esmc_300m_ss_ewc/model_esmc_300m_ss_lr_default.ckpt'\n",
    "# ckpt_path = '/net/vast-storage/scratch/vast/kellislab/artliang/magneton/finetuning/esmc_300m_ss_ewc/checkpoints_esmc_300m_ss_ewc/model_esmc_300m_ss_lr_default'\n",
    "\n",
    "# .ckpt files are less complicated\n",
    "# model = torch.load(ckpt, map_location=torch.device('cuda'))\n",
    "\n",
    "# If you trust the checkpoint source\n",
    "torch.serialization.add_safe_globals([\n",
    "    PipelineConfig,\n",
    "    # Add other classes if needed\n",
    "])\n",
    "\n",
    "model = torch.load(ckpt, map_location=torch.device('cuda'), weights_only=False)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint keys: dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers', 'hparams_name', 'hyper_parameters'])\n",
      "\n",
      "Model layers:\n",
      "fisher_info: torch.Size([332997184])\n",
      "embedder.model.embed.weight: torch.Size([64, 960])\n",
      "embedder.model.transformer.blocks.0.attn.layernorm_qkv.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.0.attn.layernorm_qkv.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.0.attn.layernorm_qkv.1.weight: torch.Size([2880, 960])\n",
      "embedder.model.transformer.blocks.0.attn.out_proj.weight: torch.Size([960, 960])\n",
      "embedder.model.transformer.blocks.0.attn.q_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.0.attn.k_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.0.ffn.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.0.ffn.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.0.ffn.1.weight: torch.Size([5120, 960])\n",
      "embedder.model.transformer.blocks.0.ffn.3.weight: torch.Size([960, 2560])\n",
      "embedder.model.transformer.blocks.1.attn.layernorm_qkv.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.1.attn.layernorm_qkv.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.1.attn.layernorm_qkv.1.weight: torch.Size([2880, 960])\n",
      "embedder.model.transformer.blocks.1.attn.out_proj.weight: torch.Size([960, 960])\n",
      "embedder.model.transformer.blocks.1.attn.q_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.1.attn.k_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.1.ffn.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.1.ffn.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.1.ffn.1.weight: torch.Size([5120, 960])\n",
      "embedder.model.transformer.blocks.1.ffn.3.weight: torch.Size([960, 2560])\n",
      "embedder.model.transformer.blocks.2.attn.layernorm_qkv.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.2.attn.layernorm_qkv.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.2.attn.layernorm_qkv.1.weight: torch.Size([2880, 960])\n",
      "embedder.model.transformer.blocks.2.attn.out_proj.weight: torch.Size([960, 960])\n",
      "embedder.model.transformer.blocks.2.attn.q_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.2.attn.k_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.2.ffn.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.2.ffn.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.2.ffn.1.weight: torch.Size([5120, 960])\n",
      "embedder.model.transformer.blocks.2.ffn.3.weight: torch.Size([960, 2560])\n",
      "embedder.model.transformer.blocks.3.attn.layernorm_qkv.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.3.attn.layernorm_qkv.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.3.attn.layernorm_qkv.1.weight: torch.Size([2880, 960])\n",
      "embedder.model.transformer.blocks.3.attn.out_proj.weight: torch.Size([960, 960])\n",
      "embedder.model.transformer.blocks.3.attn.q_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.3.attn.k_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.3.ffn.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.3.ffn.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.3.ffn.1.weight: torch.Size([5120, 960])\n",
      "embedder.model.transformer.blocks.3.ffn.3.weight: torch.Size([960, 2560])\n",
      "embedder.model.transformer.blocks.4.attn.layernorm_qkv.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.4.attn.layernorm_qkv.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.4.attn.layernorm_qkv.1.weight: torch.Size([2880, 960])\n",
      "embedder.model.transformer.blocks.4.attn.out_proj.weight: torch.Size([960, 960])\n",
      "embedder.model.transformer.blocks.4.attn.q_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.4.attn.k_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.4.ffn.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.4.ffn.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.4.ffn.1.weight: torch.Size([5120, 960])\n",
      "embedder.model.transformer.blocks.4.ffn.3.weight: torch.Size([960, 2560])\n",
      "embedder.model.transformer.blocks.5.attn.layernorm_qkv.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.5.attn.layernorm_qkv.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.5.attn.layernorm_qkv.1.weight: torch.Size([2880, 960])\n",
      "embedder.model.transformer.blocks.5.attn.out_proj.weight: torch.Size([960, 960])\n",
      "embedder.model.transformer.blocks.5.attn.q_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.5.attn.k_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.5.ffn.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.5.ffn.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.5.ffn.1.weight: torch.Size([5120, 960])\n",
      "embedder.model.transformer.blocks.5.ffn.3.weight: torch.Size([960, 2560])\n",
      "embedder.model.transformer.blocks.6.attn.layernorm_qkv.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.6.attn.layernorm_qkv.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.6.attn.layernorm_qkv.1.weight: torch.Size([2880, 960])\n",
      "embedder.model.transformer.blocks.6.attn.out_proj.weight: torch.Size([960, 960])\n",
      "embedder.model.transformer.blocks.6.attn.q_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.6.attn.k_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.6.ffn.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.6.ffn.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.6.ffn.1.weight: torch.Size([5120, 960])\n",
      "embedder.model.transformer.blocks.6.ffn.3.weight: torch.Size([960, 2560])\n",
      "embedder.model.transformer.blocks.7.attn.layernorm_qkv.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.7.attn.layernorm_qkv.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.7.attn.layernorm_qkv.1.weight: torch.Size([2880, 960])\n",
      "embedder.model.transformer.blocks.7.attn.out_proj.weight: torch.Size([960, 960])\n",
      "embedder.model.transformer.blocks.7.attn.q_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.7.attn.k_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.7.ffn.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.7.ffn.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.7.ffn.1.weight: torch.Size([5120, 960])\n",
      "embedder.model.transformer.blocks.7.ffn.3.weight: torch.Size([960, 2560])\n",
      "embedder.model.transformer.blocks.8.attn.layernorm_qkv.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.8.attn.layernorm_qkv.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.8.attn.layernorm_qkv.1.weight: torch.Size([2880, 960])\n",
      "embedder.model.transformer.blocks.8.attn.out_proj.weight: torch.Size([960, 960])\n",
      "embedder.model.transformer.blocks.8.attn.q_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.8.attn.k_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.8.ffn.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.8.ffn.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.8.ffn.1.weight: torch.Size([5120, 960])\n",
      "embedder.model.transformer.blocks.8.ffn.3.weight: torch.Size([960, 2560])\n",
      "embedder.model.transformer.blocks.9.attn.layernorm_qkv.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.9.attn.layernorm_qkv.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.9.attn.layernorm_qkv.1.weight: torch.Size([2880, 960])\n",
      "embedder.model.transformer.blocks.9.attn.out_proj.weight: torch.Size([960, 960])\n",
      "embedder.model.transformer.blocks.9.attn.q_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.9.attn.k_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.9.ffn.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.9.ffn.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.9.ffn.1.weight: torch.Size([5120, 960])\n",
      "embedder.model.transformer.blocks.9.ffn.3.weight: torch.Size([960, 2560])\n",
      "embedder.model.transformer.blocks.10.attn.layernorm_qkv.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.10.attn.layernorm_qkv.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.10.attn.layernorm_qkv.1.weight: torch.Size([2880, 960])\n",
      "embedder.model.transformer.blocks.10.attn.out_proj.weight: torch.Size([960, 960])\n",
      "embedder.model.transformer.blocks.10.attn.q_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.10.attn.k_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.10.ffn.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.10.ffn.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.10.ffn.1.weight: torch.Size([5120, 960])\n",
      "embedder.model.transformer.blocks.10.ffn.3.weight: torch.Size([960, 2560])\n",
      "embedder.model.transformer.blocks.11.attn.layernorm_qkv.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.11.attn.layernorm_qkv.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.11.attn.layernorm_qkv.1.weight: torch.Size([2880, 960])\n",
      "embedder.model.transformer.blocks.11.attn.out_proj.weight: torch.Size([960, 960])\n",
      "embedder.model.transformer.blocks.11.attn.q_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.11.attn.k_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.11.ffn.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.11.ffn.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.11.ffn.1.weight: torch.Size([5120, 960])\n",
      "embedder.model.transformer.blocks.11.ffn.3.weight: torch.Size([960, 2560])\n",
      "embedder.model.transformer.blocks.12.attn.layernorm_qkv.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.12.attn.layernorm_qkv.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.12.attn.layernorm_qkv.1.weight: torch.Size([2880, 960])\n",
      "embedder.model.transformer.blocks.12.attn.out_proj.weight: torch.Size([960, 960])\n",
      "embedder.model.transformer.blocks.12.attn.q_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.12.attn.k_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.12.ffn.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.12.ffn.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.12.ffn.1.weight: torch.Size([5120, 960])\n",
      "embedder.model.transformer.blocks.12.ffn.3.weight: torch.Size([960, 2560])\n",
      "embedder.model.transformer.blocks.13.attn.layernorm_qkv.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.13.attn.layernorm_qkv.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.13.attn.layernorm_qkv.1.weight: torch.Size([2880, 960])\n",
      "embedder.model.transformer.blocks.13.attn.out_proj.weight: torch.Size([960, 960])\n",
      "embedder.model.transformer.blocks.13.attn.q_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.13.attn.k_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.13.ffn.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.13.ffn.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.13.ffn.1.weight: torch.Size([5120, 960])\n",
      "embedder.model.transformer.blocks.13.ffn.3.weight: torch.Size([960, 2560])\n",
      "embedder.model.transformer.blocks.14.attn.layernorm_qkv.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.14.attn.layernorm_qkv.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.14.attn.layernorm_qkv.1.weight: torch.Size([2880, 960])\n",
      "embedder.model.transformer.blocks.14.attn.out_proj.weight: torch.Size([960, 960])\n",
      "embedder.model.transformer.blocks.14.attn.q_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.14.attn.k_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.14.ffn.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.14.ffn.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.14.ffn.1.weight: torch.Size([5120, 960])\n",
      "embedder.model.transformer.blocks.14.ffn.3.weight: torch.Size([960, 2560])\n",
      "embedder.model.transformer.blocks.15.attn.layernorm_qkv.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.15.attn.layernorm_qkv.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.15.attn.layernorm_qkv.1.weight: torch.Size([2880, 960])\n",
      "embedder.model.transformer.blocks.15.attn.out_proj.weight: torch.Size([960, 960])\n",
      "embedder.model.transformer.blocks.15.attn.q_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.15.attn.k_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.15.ffn.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.15.ffn.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.15.ffn.1.weight: torch.Size([5120, 960])\n",
      "embedder.model.transformer.blocks.15.ffn.3.weight: torch.Size([960, 2560])\n",
      "embedder.model.transformer.blocks.16.attn.layernorm_qkv.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.16.attn.layernorm_qkv.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.16.attn.layernorm_qkv.1.weight: torch.Size([2880, 960])\n",
      "embedder.model.transformer.blocks.16.attn.out_proj.weight: torch.Size([960, 960])\n",
      "embedder.model.transformer.blocks.16.attn.q_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.16.attn.k_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.16.ffn.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.16.ffn.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.16.ffn.1.weight: torch.Size([5120, 960])\n",
      "embedder.model.transformer.blocks.16.ffn.3.weight: torch.Size([960, 2560])\n",
      "embedder.model.transformer.blocks.17.attn.layernorm_qkv.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.17.attn.layernorm_qkv.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.17.attn.layernorm_qkv.1.weight: torch.Size([2880, 960])\n",
      "embedder.model.transformer.blocks.17.attn.out_proj.weight: torch.Size([960, 960])\n",
      "embedder.model.transformer.blocks.17.attn.q_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.17.attn.k_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.17.ffn.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.17.ffn.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.17.ffn.1.weight: torch.Size([5120, 960])\n",
      "embedder.model.transformer.blocks.17.ffn.3.weight: torch.Size([960, 2560])\n",
      "embedder.model.transformer.blocks.18.attn.layernorm_qkv.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.18.attn.layernorm_qkv.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.18.attn.layernorm_qkv.1.weight: torch.Size([2880, 960])\n",
      "embedder.model.transformer.blocks.18.attn.out_proj.weight: torch.Size([960, 960])\n",
      "embedder.model.transformer.blocks.18.attn.q_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.18.attn.k_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.18.ffn.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.18.ffn.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.18.ffn.1.weight: torch.Size([5120, 960])\n",
      "embedder.model.transformer.blocks.18.ffn.3.weight: torch.Size([960, 2560])\n",
      "embedder.model.transformer.blocks.19.attn.layernorm_qkv.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.19.attn.layernorm_qkv.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.19.attn.layernorm_qkv.1.weight: torch.Size([2880, 960])\n",
      "embedder.model.transformer.blocks.19.attn.out_proj.weight: torch.Size([960, 960])\n",
      "embedder.model.transformer.blocks.19.attn.q_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.19.attn.k_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.19.ffn.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.19.ffn.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.19.ffn.1.weight: torch.Size([5120, 960])\n",
      "embedder.model.transformer.blocks.19.ffn.3.weight: torch.Size([960, 2560])\n",
      "embedder.model.transformer.blocks.20.attn.layernorm_qkv.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.20.attn.layernorm_qkv.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.20.attn.layernorm_qkv.1.weight: torch.Size([2880, 960])\n",
      "embedder.model.transformer.blocks.20.attn.out_proj.weight: torch.Size([960, 960])\n",
      "embedder.model.transformer.blocks.20.attn.q_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.20.attn.k_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.20.ffn.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.20.ffn.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.20.ffn.1.weight: torch.Size([5120, 960])\n",
      "embedder.model.transformer.blocks.20.ffn.3.weight: torch.Size([960, 2560])\n",
      "embedder.model.transformer.blocks.21.attn.layernorm_qkv.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.21.attn.layernorm_qkv.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.21.attn.layernorm_qkv.1.weight: torch.Size([2880, 960])\n",
      "embedder.model.transformer.blocks.21.attn.out_proj.weight: torch.Size([960, 960])\n",
      "embedder.model.transformer.blocks.21.attn.q_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.21.attn.k_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.21.ffn.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.21.ffn.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.21.ffn.1.weight: torch.Size([5120, 960])\n",
      "embedder.model.transformer.blocks.21.ffn.3.weight: torch.Size([960, 2560])\n",
      "embedder.model.transformer.blocks.22.attn.layernorm_qkv.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.22.attn.layernorm_qkv.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.22.attn.layernorm_qkv.1.weight: torch.Size([2880, 960])\n",
      "embedder.model.transformer.blocks.22.attn.out_proj.weight: torch.Size([960, 960])\n",
      "embedder.model.transformer.blocks.22.attn.q_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.22.attn.k_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.22.ffn.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.22.ffn.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.22.ffn.1.weight: torch.Size([5120, 960])\n",
      "embedder.model.transformer.blocks.22.ffn.3.weight: torch.Size([960, 2560])\n",
      "embedder.model.transformer.blocks.23.attn.layernorm_qkv.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.23.attn.layernorm_qkv.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.23.attn.layernorm_qkv.1.weight: torch.Size([2880, 960])\n",
      "embedder.model.transformer.blocks.23.attn.out_proj.weight: torch.Size([960, 960])\n",
      "embedder.model.transformer.blocks.23.attn.q_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.23.attn.k_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.23.ffn.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.23.ffn.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.23.ffn.1.weight: torch.Size([5120, 960])\n",
      "embedder.model.transformer.blocks.23.ffn.3.weight: torch.Size([960, 2560])\n",
      "embedder.model.transformer.blocks.24.attn.layernorm_qkv.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.24.attn.layernorm_qkv.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.24.attn.layernorm_qkv.1.weight: torch.Size([2880, 960])\n",
      "embedder.model.transformer.blocks.24.attn.out_proj.weight: torch.Size([960, 960])\n",
      "embedder.model.transformer.blocks.24.attn.q_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.24.attn.k_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.24.ffn.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.24.ffn.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.24.ffn.1.weight: torch.Size([5120, 960])\n",
      "embedder.model.transformer.blocks.24.ffn.3.weight: torch.Size([960, 2560])\n",
      "embedder.model.transformer.blocks.25.attn.layernorm_qkv.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.25.attn.layernorm_qkv.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.25.attn.layernorm_qkv.1.weight: torch.Size([2880, 960])\n",
      "embedder.model.transformer.blocks.25.attn.out_proj.weight: torch.Size([960, 960])\n",
      "embedder.model.transformer.blocks.25.attn.q_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.25.attn.k_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.25.ffn.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.25.ffn.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.25.ffn.1.weight: torch.Size([5120, 960])\n",
      "embedder.model.transformer.blocks.25.ffn.3.weight: torch.Size([960, 2560])\n",
      "embedder.model.transformer.blocks.26.attn.layernorm_qkv.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.26.attn.layernorm_qkv.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.26.attn.layernorm_qkv.1.weight: torch.Size([2880, 960])\n",
      "embedder.model.transformer.blocks.26.attn.out_proj.weight: torch.Size([960, 960])\n",
      "embedder.model.transformer.blocks.26.attn.q_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.26.attn.k_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.26.ffn.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.26.ffn.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.26.ffn.1.weight: torch.Size([5120, 960])\n",
      "embedder.model.transformer.blocks.26.ffn.3.weight: torch.Size([960, 2560])\n",
      "embedder.model.transformer.blocks.27.attn.layernorm_qkv.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.27.attn.layernorm_qkv.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.27.attn.layernorm_qkv.1.weight: torch.Size([2880, 960])\n",
      "embedder.model.transformer.blocks.27.attn.out_proj.weight: torch.Size([960, 960])\n",
      "embedder.model.transformer.blocks.27.attn.q_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.27.attn.k_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.27.ffn.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.27.ffn.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.27.ffn.1.weight: torch.Size([5120, 960])\n",
      "embedder.model.transformer.blocks.27.ffn.3.weight: torch.Size([960, 2560])\n",
      "embedder.model.transformer.blocks.28.attn.layernorm_qkv.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.28.attn.layernorm_qkv.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.28.attn.layernorm_qkv.1.weight: torch.Size([2880, 960])\n",
      "embedder.model.transformer.blocks.28.attn.out_proj.weight: torch.Size([960, 960])\n",
      "embedder.model.transformer.blocks.28.attn.q_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.28.attn.k_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.28.ffn.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.28.ffn.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.28.ffn.1.weight: torch.Size([5120, 960])\n",
      "embedder.model.transformer.blocks.28.ffn.3.weight: torch.Size([960, 2560])\n",
      "embedder.model.transformer.blocks.29.attn.layernorm_qkv.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.29.attn.layernorm_qkv.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.29.attn.layernorm_qkv.1.weight: torch.Size([2880, 960])\n",
      "embedder.model.transformer.blocks.29.attn.out_proj.weight: torch.Size([960, 960])\n",
      "embedder.model.transformer.blocks.29.attn.q_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.29.attn.k_ln.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.29.ffn.0.weight: torch.Size([960])\n",
      "embedder.model.transformer.blocks.29.ffn.0.bias: torch.Size([960])\n",
      "embedder.model.transformer.blocks.29.ffn.1.weight: torch.Size([5120, 960])\n",
      "embedder.model.transformer.blocks.29.ffn.3.weight: torch.Size([960, 2560])\n",
      "embedder.model.transformer.norm.weight: torch.Size([960])\n",
      "embedder.model.sequence_head.0.weight: torch.Size([960, 960])\n",
      "embedder.model.sequence_head.0.bias: torch.Size([960])\n",
      "embedder.model.sequence_head.2.weight: torch.Size([960])\n",
      "embedder.model.sequence_head.2.bias: torch.Size([960])\n",
      "embedder.model.sequence_head.3.weight: torch.Size([64, 960])\n",
      "embedder.model.sequence_head.3.bias: torch.Size([64])\n",
      "mlp.0.weight: torch.Size([256, 960])\n",
      "mlp.0.bias: torch.Size([256])\n",
      "mlp.3.weight: torch.Size([256, 256])\n",
      "mlp.3.bias: torch.Size([256])\n",
      "mlp.6.weight: torch.Size([256, 256])\n",
      "mlp.6.bias: torch.Size([256])\n",
      "mlp.9.weight: torch.Size([9, 256])\n",
      "mlp.9.bias: torch.Size([9])\n"
     ]
    }
   ],
   "source": [
    "# See what's in the model\n",
    "print(\"Checkpoint keys:\", model.keys())\n",
    "\n",
    "# If it's a PyTorch Lightning model, the model is usually in 'state_dict'\n",
    "if 'state_dict' in model:\n",
    "    state_dict = model['state_dict']\n",
    "    print(\"\\nModel layers:\")\n",
    "    for name, param in state_dict.items():\n",
    "        print(f\"{name}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing: fisher_info\n",
      "Removing: mlp.0.weight\n",
      "Removing: mlp.0.bias\n",
      "Removing: mlp.3.weight\n",
      "Removing: mlp.3.bias\n",
      "Removing: mlp.6.weight\n",
      "Removing: mlp.6.bias\n",
      "Removing: mlp.9.weight\n",
      "Removing: mlp.9.bias\n"
     ]
    }
   ],
   "source": [
    "# Create a new state_dict without MLP layers\n",
    "new_state_dict = {}\n",
    "\n",
    "for name, param in model['state_dict'].items():\n",
    "    # Skip any parameter that contains 'mlp' in its name (and fisher in the case of EWC checkpoints)\n",
    "    if 'mlp' not in name.lower() and 'fisher' not in name.lower():\n",
    "        new_state_dict[name] = param\n",
    "    else:\n",
    "        print(f\"Removing: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esm.models.esmc import ESMC\n",
    "\n",
    "esmc = ESMC.from_pretrained(\"esmc_300m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'transformer.blocks.12.attn.k_ln.weight', 'transformer.blocks.14.ffn.0.bias', 'transformer.blocks.22.attn.k_ln.weight', 'transformer.blocks.16.attn.layernorm_qkv.0.bias', 'transformer.blocks.11.ffn.3.weight', 'transformer.blocks.9.ffn.3.weight', 'transformer.blocks.11.attn.q_ln.weight', 'transformer.blocks.26.ffn.0.bias', 'transformer.blocks.8.ffn.3.weight', 'transformer.blocks.23.attn.k_ln.weight', 'transformer.blocks.18.attn.out_proj.weight', 'transformer.blocks.15.attn.k_ln.weight', 'transformer.blocks.27.ffn.0.weight', 'transformer.blocks.29.ffn.3.weight', 'sequence_head.0.weight', 'transformer.blocks.1.attn.layernorm_qkv.0.bias', 'transformer.blocks.5.attn.out_proj.weight', 'transformer.blocks.3.attn.layernorm_qkv.1.weight', 'transformer.blocks.21.attn.layernorm_qkv.0.weight', 'transformer.blocks.26.ffn.1.weight', 'transformer.blocks.9.attn.q_ln.weight', 'transformer.blocks.18.attn.layernorm_qkv.0.weight', 'transformer.blocks.18.attn.q_ln.weight', 'transformer.blocks.25.attn.layernorm_qkv.0.weight', 'transformer.blocks.12.attn.layernorm_qkv.0.bias', 'transformer.blocks.27.attn.q_ln.weight', 'transformer.blocks.26.attn.layernorm_qkv.1.weight', 'transformer.blocks.5.ffn.3.weight', 'transformer.blocks.16.ffn.0.bias', 'transformer.blocks.20.attn.layernorm_qkv.1.weight', 'transformer.blocks.16.attn.layernorm_qkv.1.weight', 'transformer.blocks.20.ffn.1.weight', 'transformer.blocks.24.ffn.3.weight', 'transformer.blocks.24.ffn.0.bias', 'transformer.blocks.10.attn.out_proj.weight', 'transformer.blocks.2.ffn.1.weight', 'transformer.blocks.14.attn.layernorm_qkv.1.weight', 'transformer.blocks.28.ffn.0.weight', 'transformer.blocks.10.attn.layernorm_qkv.0.weight', 'transformer.blocks.7.ffn.0.bias', 'transformer.blocks.9.ffn.1.weight', 'transformer.blocks.17.ffn.0.weight', 'transformer.blocks.13.attn.layernorm_qkv.0.weight', 'transformer.blocks.13.attn.layernorm_qkv.0.bias', 'transformer.blocks.11.attn.layernorm_qkv.1.weight', 'transformer.blocks.9.ffn.0.bias', 'transformer.blocks.18.ffn.1.weight', 'transformer.blocks.22.attn.layernorm_qkv.0.weight', 'transformer.blocks.8.ffn.1.weight', 'transformer.blocks.10.ffn.1.weight', 'transformer.blocks.9.attn.layernorm_qkv.0.weight', 'transformer.blocks.21.ffn.1.weight', 'transformer.blocks.7.attn.k_ln.weight', 'transformer.blocks.17.attn.layernorm_qkv.0.weight', 'transformer.blocks.10.ffn.3.weight', 'transformer.blocks.1.attn.k_ln.weight', 'transformer.blocks.21.attn.layernorm_qkv.1.weight', 'transformer.blocks.27.attn.out_proj.weight', 'transformer.blocks.9.attn.out_proj.weight', 'transformer.blocks.16.ffn.3.weight', 'transformer.blocks.5.ffn.1.weight', 'transformer.blocks.6.ffn.1.weight', 'transformer.blocks.14.attn.layernorm_qkv.0.bias', 'transformer.blocks.12.attn.q_ln.weight', 'transformer.blocks.2.attn.out_proj.weight', 'transformer.blocks.22.attn.q_ln.weight', 'transformer.blocks.8.ffn.0.bias', 'sequence_head.0.bias', 'transformer.blocks.23.ffn.3.weight', 'transformer.blocks.24.attn.k_ln.weight', 'sequence_head.2.bias', 'transformer.blocks.17.attn.layernorm_qkv.0.bias', 'transformer.blocks.28.attn.out_proj.weight', 'transformer.blocks.3.attn.out_proj.weight', 'transformer.blocks.15.attn.out_proj.weight', 'transformer.blocks.6.attn.layernorm_qkv.0.weight', 'transformer.blocks.29.ffn.1.weight', 'transformer.blocks.5.attn.layernorm_qkv.1.weight', 'transformer.blocks.19.ffn.3.weight', 'transformer.blocks.25.attn.k_ln.weight', 'transformer.blocks.2.attn.layernorm_qkv.0.weight', 'transformer.blocks.1.attn.layernorm_qkv.1.weight', 'transformer.blocks.24.attn.layernorm_qkv.1.weight', 'transformer.blocks.0.ffn.0.bias', 'transformer.blocks.16.attn.q_ln.weight', 'transformer.blocks.17.attn.q_ln.weight', 'transformer.blocks.21.attn.out_proj.weight', 'transformer.blocks.29.attn.out_proj.weight', 'transformer.blocks.23.ffn.0.bias', 'transformer.blocks.29.attn.layernorm_qkv.0.weight', 'transformer.blocks.18.ffn.0.bias', 'transformer.blocks.4.attn.out_proj.weight', 'transformer.blocks.12.ffn.1.weight', 'transformer.blocks.3.ffn.0.bias', 'transformer.blocks.16.ffn.0.weight', 'transformer.blocks.27.attn.k_ln.weight', 'transformer.blocks.4.attn.layernorm_qkv.0.bias', 'sequence_head.3.bias', 'transformer.blocks.7.attn.layernorm_qkv.0.weight', 'transformer.blocks.14.ffn.0.weight', 'transformer.blocks.2.ffn.3.weight', 'transformer.blocks.3.ffn.1.weight', 'transformer.blocks.22.attn.layernorm_qkv.0.bias', 'transformer.blocks.19.attn.layernorm_qkv.0.bias', 'transformer.blocks.0.attn.layernorm_qkv.1.weight', 'transformer.blocks.2.attn.layernorm_qkv.0.bias', 'transformer.blocks.17.attn.layernorm_qkv.1.weight', 'transformer.blocks.2.ffn.0.bias', 'transformer.blocks.8.attn.k_ln.weight', 'transformer.blocks.16.attn.k_ln.weight', 'transformer.blocks.29.attn.layernorm_qkv.1.weight', 'transformer.blocks.1.attn.layernorm_qkv.0.weight', 'transformer.blocks.9.attn.layernorm_qkv.0.bias', 'transformer.blocks.12.ffn.0.bias', 'transformer.blocks.4.ffn.0.bias', 'transformer.blocks.20.attn.k_ln.weight', 'transformer.blocks.23.attn.out_proj.weight', 'transformer.blocks.23.attn.q_ln.weight', 'transformer.blocks.22.ffn.1.weight', 'transformer.blocks.13.attn.out_proj.weight', 'transformer.blocks.21.attn.q_ln.weight', 'transformer.blocks.25.attn.layernorm_qkv.1.weight', 'transformer.blocks.11.attn.out_proj.weight', 'transformer.blocks.13.attn.q_ln.weight', 'transformer.blocks.3.ffn.0.weight', 'transformer.blocks.5.attn.k_ln.weight', 'transformer.blocks.18.attn.k_ln.weight', 'transformer.blocks.12.attn.layernorm_qkv.0.weight', 'transformer.blocks.5.attn.q_ln.weight', 'transformer.blocks.15.attn.layernorm_qkv.1.weight', 'transformer.blocks.14.attn.q_ln.weight', 'transformer.blocks.19.ffn.1.weight', 'transformer.blocks.19.ffn.0.weight', 'transformer.blocks.0.ffn.1.weight', 'transformer.blocks.0.attn.layernorm_qkv.0.bias', 'sequence_head.2.weight', 'transformer.blocks.15.attn.layernorm_qkv.0.weight', 'transformer.blocks.6.attn.layernorm_qkv.0.bias', 'transformer.blocks.17.ffn.3.weight', 'transformer.blocks.0.ffn.3.weight', 'transformer.blocks.2.attn.q_ln.weight', 'transformer.blocks.26.attn.layernorm_qkv.0.bias', 'transformer.blocks.27.attn.layernorm_qkv.0.weight', 'transformer.blocks.15.ffn.3.weight', 'transformer.blocks.19.attn.layernorm_qkv.1.weight', 'transformer.blocks.12.attn.layernorm_qkv.1.weight', 'transformer.blocks.28.ffn.1.weight', 'transformer.blocks.22.ffn.0.weight', 'transformer.blocks.27.ffn.0.bias', 'transformer.blocks.14.ffn.1.weight', 'transformer.blocks.10.ffn.0.bias', 'transformer.blocks.23.attn.layernorm_qkv.1.weight', 'transformer.blocks.1.attn.q_ln.weight', 'transformer.blocks.10.attn.q_ln.weight', 'transformer.blocks.20.ffn.3.weight', 'transformer.blocks.3.attn.layernorm_qkv.0.weight', 'transformer.blocks.27.attn.layernorm_qkv.0.bias', 'transformer.blocks.22.attn.out_proj.weight', 'transformer.blocks.19.attn.layernorm_qkv.0.weight', 'transformer.blocks.15.attn.layernorm_qkv.0.bias', 'transformer.blocks.13.ffn.0.bias', 'transformer.blocks.16.ffn.1.weight', 'transformer.blocks.20.attn.layernorm_qkv.0.weight', 'transformer.blocks.11.ffn.1.weight', 'transformer.blocks.26.ffn.3.weight', 'transformer.blocks.5.ffn.0.bias', 'transformer.blocks.1.ffn.0.bias', 'transformer.blocks.6.attn.q_ln.weight', 'transformer.blocks.4.ffn.1.weight', 'transformer.blocks.21.ffn.0.bias', 'transformer.blocks.28.ffn.0.bias', 'transformer.blocks.21.attn.layernorm_qkv.0.bias', 'transformer.blocks.17.attn.k_ln.weight', 'transformer.blocks.7.ffn.3.weight', 'transformer.blocks.8.attn.layernorm_qkv.1.weight', 'transformer.blocks.18.ffn.0.weight', 'transformer.blocks.25.ffn.0.weight', 'transformer.blocks.6.attn.k_ln.weight', 'transformer.blocks.28.attn.layernorm_qkv.1.weight', 'transformer.blocks.17.attn.out_proj.weight', 'transformer.blocks.23.attn.layernorm_qkv.0.weight', 'transformer.blocks.25.attn.layernorm_qkv.0.bias', 'transformer.blocks.10.ffn.0.weight', 'transformer.blocks.14.attn.layernorm_qkv.0.weight', 'transformer.blocks.26.attn.q_ln.weight', 'transformer.blocks.13.ffn.1.weight', 'transformer.blocks.5.attn.layernorm_qkv.0.weight', 'transformer.blocks.4.ffn.0.weight', 'transformer.blocks.28.attn.layernorm_qkv.0.weight', 'transformer.blocks.3.attn.layernorm_qkv.0.bias', 'transformer.blocks.0.ffn.0.weight', 'transformer.blocks.19.attn.q_ln.weight', 'transformer.blocks.25.attn.q_ln.weight', 'transformer.blocks.24.ffn.1.weight', 'embed.weight', 'transformer.blocks.11.ffn.0.weight', 'transformer.blocks.2.attn.k_ln.weight', 'transformer.blocks.19.ffn.0.bias', 'transformer.blocks.7.ffn.0.weight', 'transformer.blocks.15.ffn.1.weight', 'transformer.blocks.11.ffn.0.bias', 'transformer.blocks.4.ffn.3.weight', 'transformer.blocks.26.ffn.0.weight', 'transformer.blocks.24.attn.layernorm_qkv.0.weight', 'transformer.blocks.9.attn.k_ln.weight', 'transformer.blocks.20.ffn.0.bias', 'transformer.blocks.13.ffn.3.weight', 'transformer.blocks.16.attn.layernorm_qkv.0.weight', 'transformer.blocks.24.attn.out_proj.weight', 'transformer.blocks.12.ffn.3.weight', 'transformer.blocks.0.attn.q_ln.weight', 'transformer.blocks.16.attn.out_proj.weight', 'transformer.blocks.15.ffn.0.bias', 'transformer.blocks.28.attn.q_ln.weight', 'transformer.blocks.4.attn.q_ln.weight', 'transformer.blocks.6.attn.out_proj.weight', 'transformer.blocks.18.attn.layernorm_qkv.1.weight', 'transformer.blocks.5.ffn.0.weight', 'transformer.blocks.18.ffn.3.weight', 'transformer.blocks.3.attn.k_ln.weight', 'sequence_head.3.weight', 'transformer.blocks.19.attn.k_ln.weight', 'transformer.blocks.22.attn.layernorm_qkv.1.weight', 'transformer.blocks.22.ffn.3.weight', 'transformer.blocks.25.ffn.3.weight', 'transformer.blocks.7.attn.out_proj.weight', 'transformer.blocks.29.ffn.0.weight', 'transformer.blocks.9.attn.layernorm_qkv.1.weight', 'transformer.blocks.1.attn.out_proj.weight', 'transformer.blocks.6.ffn.3.weight', 'transformer.blocks.24.attn.layernorm_qkv.0.bias', 'transformer.blocks.27.ffn.3.weight', 'transformer.blocks.24.attn.q_ln.weight', 'transformer.blocks.20.ffn.0.weight', 'transformer.blocks.0.attn.k_ln.weight', 'transformer.blocks.4.attn.k_ln.weight', 'transformer.blocks.6.attn.layernorm_qkv.1.weight', 'transformer.blocks.23.attn.layernorm_qkv.0.bias', 'transformer.blocks.28.ffn.3.weight', 'transformer.blocks.4.attn.layernorm_qkv.1.weight', 'transformer.blocks.20.attn.q_ln.weight', 'transformer.blocks.7.ffn.1.weight', 'transformer.blocks.13.attn.layernorm_qkv.1.weight', 'transformer.blocks.17.ffn.1.weight', 'transformer.blocks.29.ffn.0.bias', 'transformer.norm.weight', 'transformer.blocks.28.attn.k_ln.weight', 'transformer.blocks.14.attn.out_proj.weight', 'transformer.blocks.3.ffn.3.weight', 'transformer.blocks.10.attn.k_ln.weight', 'transformer.blocks.7.attn.q_ln.weight', 'transformer.blocks.10.attn.layernorm_qkv.1.weight', 'transformer.blocks.13.attn.k_ln.weight', 'transformer.blocks.14.ffn.3.weight', 'transformer.blocks.8.ffn.0.weight', 'transformer.blocks.7.attn.layernorm_qkv.1.weight', 'transformer.blocks.18.attn.layernorm_qkv.0.bias', 'transformer.blocks.11.attn.layernorm_qkv.0.weight', 'transformer.blocks.12.attn.out_proj.weight', 'transformer.blocks.26.attn.k_ln.weight', 'transformer.blocks.7.attn.layernorm_qkv.0.bias', 'transformer.blocks.9.ffn.0.weight', 'transformer.blocks.4.attn.layernorm_qkv.0.weight', 'transformer.blocks.1.ffn.3.weight', 'transformer.blocks.23.ffn.1.weight', 'transformer.blocks.17.ffn.0.bias', 'transformer.blocks.29.attn.k_ln.weight', 'transformer.blocks.5.attn.layernorm_qkv.0.bias', 'transformer.blocks.27.ffn.1.weight', 'transformer.blocks.24.ffn.0.weight', 'transformer.blocks.13.ffn.0.weight', 'transformer.blocks.15.attn.q_ln.weight', 'transformer.blocks.10.attn.layernorm_qkv.0.bias', 'transformer.blocks.12.ffn.0.weight', 'transformer.blocks.15.ffn.0.weight', 'transformer.blocks.6.ffn.0.bias', 'transformer.blocks.2.ffn.0.weight', 'transformer.blocks.22.ffn.0.bias', 'transformer.blocks.27.attn.layernorm_qkv.1.weight', 'transformer.blocks.21.ffn.0.weight', 'transformer.blocks.11.attn.layernorm_qkv.0.bias', 'transformer.blocks.26.attn.out_proj.weight', 'transformer.blocks.3.attn.q_ln.weight', 'transformer.blocks.21.ffn.3.weight', 'transformer.blocks.28.attn.layernorm_qkv.0.bias', 'transformer.blocks.8.attn.q_ln.weight', 'transformer.blocks.20.attn.layernorm_qkv.0.bias', 'transformer.blocks.25.ffn.1.weight', 'transformer.blocks.29.attn.layernorm_qkv.0.bias', 'transformer.blocks.8.attn.layernorm_qkv.0.bias', 'transformer.blocks.14.attn.k_ln.weight', 'transformer.blocks.2.attn.layernorm_qkv.1.weight', 'transformer.blocks.1.ffn.1.weight', 'transformer.blocks.0.attn.out_proj.weight', 'transformer.blocks.1.ffn.0.weight', 'transformer.blocks.23.ffn.0.weight', 'transformer.blocks.25.attn.out_proj.weight', 'transformer.blocks.8.attn.out_proj.weight', 'transformer.blocks.29.attn.q_ln.weight', 'transformer.blocks.6.ffn.0.weight', 'transformer.blocks.25.ffn.0.bias', 'transformer.blocks.20.attn.out_proj.weight', 'transformer.blocks.8.attn.layernorm_qkv.0.weight', 'transformer.blocks.21.attn.k_ln.weight', 'transformer.blocks.0.attn.layernorm_qkv.0.weight', 'transformer.blocks.26.attn.layernorm_qkv.0.weight', 'transformer.blocks.19.attn.out_proj.weight', 'transformer.blocks.11.attn.k_ln.weight'}\n",
      "{'embedder.model.transformer.blocks.29.attn.layernorm_qkv.0.weight', 'embedder.model.transformer.blocks.5.ffn.0.weight', 'embedder.model.transformer.blocks.5.ffn.1.weight', 'embedder.model.transformer.blocks.4.attn.q_ln.weight', 'embedder.model.transformer.blocks.28.attn.q_ln.weight', 'embedder.model.transformer.blocks.1.attn.q_ln.weight', 'embedder.model.transformer.blocks.13.attn.q_ln.weight', 'embedder.model.sequence_head.0.bias', 'embedder.model.transformer.blocks.3.attn.layernorm_qkv.0.bias', 'embedder.model.transformer.blocks.5.attn.layernorm_qkv.1.weight', 'embedder.model.transformer.blocks.19.attn.out_proj.weight', 'embedder.model.transformer.blocks.29.attn.out_proj.weight', 'embedder.model.transformer.blocks.21.ffn.0.bias', 'embedder.model.transformer.blocks.3.ffn.0.bias', 'embedder.model.transformer.blocks.10.ffn.0.bias', 'embedder.model.transformer.blocks.22.attn.k_ln.weight', 'embedder.model.transformer.blocks.28.attn.out_proj.weight', 'embedder.model.transformer.blocks.17.attn.q_ln.weight', 'embedder.model.transformer.blocks.16.attn.k_ln.weight', 'embedder.model.transformer.blocks.23.attn.layernorm_qkv.0.weight', 'embedder.model.transformer.blocks.5.ffn.0.bias', 'embedder.model.transformer.blocks.27.attn.out_proj.weight', 'embedder.model.transformer.blocks.13.ffn.3.weight', 'embedder.model.transformer.blocks.25.attn.layernorm_qkv.0.weight', 'embedder.model.transformer.blocks.18.attn.k_ln.weight', 'embedder.model.transformer.blocks.4.attn.out_proj.weight', 'embedder.model.transformer.blocks.4.attn.layernorm_qkv.1.weight', 'embedder.model.transformer.blocks.0.attn.q_ln.weight', 'embedder.model.transformer.blocks.3.ffn.0.weight', 'embedder.model.transformer.blocks.24.ffn.0.weight', 'embedder.model.transformer.blocks.14.attn.layernorm_qkv.0.weight', 'embedder.model.transformer.blocks.27.attn.layernorm_qkv.1.weight', 'embedder.model.transformer.blocks.20.attn.q_ln.weight', 'embedder.model.transformer.blocks.1.ffn.3.weight', 'embedder.model.transformer.blocks.2.attn.layernorm_qkv.0.bias', 'embedder.model.sequence_head.3.bias', 'embedder.model.transformer.blocks.28.attn.k_ln.weight', 'embedder.model.transformer.blocks.20.attn.layernorm_qkv.0.weight', 'embedder.model.transformer.blocks.14.attn.out_proj.weight', 'embedder.model.transformer.blocks.16.attn.q_ln.weight', 'embedder.model.transformer.blocks.28.ffn.1.weight', 'embedder.model.transformer.blocks.12.attn.layernorm_qkv.0.weight', 'embedder.model.transformer.blocks.24.ffn.3.weight', 'embedder.model.embed.weight', 'embedder.model.transformer.blocks.10.attn.k_ln.weight', 'embedder.model.transformer.blocks.12.attn.layernorm_qkv.0.bias', 'embedder.model.transformer.blocks.28.ffn.0.bias', 'embedder.model.transformer.blocks.19.attn.layernorm_qkv.1.weight', 'embedder.model.sequence_head.2.weight', 'embedder.model.transformer.blocks.2.attn.k_ln.weight', 'embedder.model.transformer.blocks.18.attn.q_ln.weight', 'embedder.model.transformer.blocks.16.attn.out_proj.weight', 'embedder.model.transformer.blocks.13.ffn.0.weight', 'embedder.model.sequence_head.2.bias', 'embedder.model.transformer.blocks.6.ffn.1.weight', 'embedder.model.transformer.blocks.7.attn.out_proj.weight', 'embedder.model.transformer.blocks.20.attn.out_proj.weight', 'embedder.model.transformer.blocks.22.ffn.0.weight', 'embedder.model.transformer.blocks.21.ffn.0.weight', 'embedder.model.transformer.blocks.27.ffn.0.bias', 'embedder.model.transformer.blocks.27.attn.layernorm_qkv.0.bias', 'embedder.model.transformer.blocks.1.ffn.1.weight', 'embedder.model.transformer.blocks.5.attn.q_ln.weight', 'embedder.model.transformer.blocks.27.ffn.1.weight', 'embedder.model.transformer.blocks.12.ffn.0.weight', 'embedder.model.transformer.blocks.20.ffn.0.weight', 'embedder.model.transformer.blocks.22.ffn.0.bias', 'embedder.model.transformer.blocks.29.attn.q_ln.weight', 'embedder.model.transformer.blocks.17.ffn.3.weight', 'embedder.model.transformer.blocks.20.attn.layernorm_qkv.1.weight', 'embedder.model.transformer.blocks.22.attn.layernorm_qkv.0.weight', 'embedder.model.transformer.blocks.15.ffn.0.bias', 'embedder.model.transformer.blocks.21.attn.layernorm_qkv.0.bias', 'embedder.model.transformer.blocks.18.ffn.3.weight', 'embedder.model.transformer.blocks.10.ffn.0.weight', 'embedder.model.transformer.blocks.21.attn.k_ln.weight', 'embedder.model.transformer.blocks.4.attn.k_ln.weight', 'embedder.model.transformer.blocks.24.ffn.0.bias', 'embedder.model.transformer.blocks.13.attn.k_ln.weight', 'embedder.model.transformer.blocks.6.attn.out_proj.weight', 'embedder.model.transformer.blocks.16.ffn.1.weight', 'embedder.model.transformer.blocks.27.attn.q_ln.weight', 'embedder.model.transformer.blocks.16.attn.layernorm_qkv.0.bias', 'embedder.model.transformer.blocks.9.attn.layernorm_qkv.1.weight', 'embedder.model.transformer.blocks.24.attn.layernorm_qkv.0.bias', 'embedder.model.transformer.blocks.6.ffn.0.weight', 'embedder.model.transformer.blocks.25.attn.q_ln.weight', 'embedder.model.transformer.blocks.14.ffn.0.bias', 'embedder.model.transformer.blocks.10.ffn.3.weight', 'embedder.model.transformer.blocks.15.attn.q_ln.weight', 'embedder.model.transformer.blocks.17.ffn.1.weight', 'embedder.model.transformer.blocks.9.attn.q_ln.weight', 'embedder.model.transformer.blocks.2.attn.out_proj.weight', 'embedder.model.transformer.blocks.0.ffn.0.bias', 'embedder.model.transformer.blocks.15.attn.layernorm_qkv.0.bias', 'embedder.model.transformer.blocks.20.attn.layernorm_qkv.0.bias', 'embedder.model.transformer.blocks.19.attn.layernorm_qkv.0.weight', 'embedder.model.transformer.blocks.29.ffn.0.bias', 'embedder.model.transformer.blocks.4.ffn.3.weight', 'embedder.model.transformer.blocks.11.ffn.1.weight', 'embedder.model.transformer.blocks.25.attn.layernorm_qkv.0.bias', 'embedder.model.transformer.blocks.20.ffn.3.weight', 'embedder.model.transformer.blocks.4.ffn.1.weight', 'embedder.model.transformer.blocks.19.ffn.3.weight', 'embedder.model.transformer.blocks.14.ffn.1.weight', 'embedder.model.transformer.blocks.2.ffn.3.weight', 'embedder.model.transformer.blocks.5.attn.layernorm_qkv.0.bias', 'embedder.model.transformer.blocks.8.attn.q_ln.weight', 'embedder.model.transformer.blocks.7.ffn.3.weight', 'embedder.model.transformer.blocks.9.ffn.3.weight', 'embedder.model.transformer.blocks.2.ffn.0.weight', 'embedder.model.sequence_head.3.weight', 'embedder.model.transformer.blocks.3.attn.layernorm_qkv.1.weight', 'embedder.model.transformer.blocks.15.ffn.1.weight', 'embedder.model.transformer.blocks.9.ffn.0.weight', 'embedder.model.transformer.blocks.22.attn.q_ln.weight', 'embedder.model.transformer.blocks.14.ffn.0.weight', 'embedder.model.transformer.blocks.8.attn.out_proj.weight', 'embedder.model.transformer.blocks.16.ffn.3.weight', 'embedder.model.transformer.blocks.26.ffn.0.bias', 'embedder.model.transformer.blocks.9.attn.k_ln.weight', 'embedder.model.transformer.blocks.19.ffn.1.weight', 'embedder.model.transformer.blocks.21.ffn.1.weight', 'embedder.model.transformer.blocks.1.ffn.0.weight', 'embedder.model.transformer.blocks.18.attn.layernorm_qkv.0.bias', 'embedder.model.transformer.blocks.16.ffn.0.bias', 'embedder.model.transformer.blocks.22.attn.layernorm_qkv.0.bias', 'embedder.model.transformer.blocks.8.attn.k_ln.weight', 'embedder.model.transformer.blocks.17.ffn.0.bias', 'embedder.model.transformer.blocks.9.attn.out_proj.weight', 'embedder.model.transformer.blocks.4.attn.layernorm_qkv.0.bias', 'embedder.model.transformer.blocks.26.attn.layernorm_qkv.0.weight', 'embedder.model.transformer.blocks.7.attn.layernorm_qkv.1.weight', 'embedder.model.transformer.blocks.11.attn.out_proj.weight', 'embedder.model.transformer.blocks.6.ffn.0.bias', 'embedder.model.transformer.blocks.14.attn.layernorm_qkv.1.weight', 'embedder.model.transformer.blocks.28.ffn.3.weight', 'embedder.model.transformer.blocks.8.attn.layernorm_qkv.0.weight', 'embedder.model.transformer.blocks.27.ffn.3.weight', 'embedder.model.transformer.blocks.7.ffn.0.bias', 'embedder.model.transformer.blocks.12.ffn.3.weight', 'embedder.model.transformer.blocks.8.ffn.1.weight', 'embedder.model.transformer.blocks.11.ffn.0.bias', 'embedder.model.transformer.blocks.6.ffn.3.weight', 'embedder.model.transformer.blocks.12.ffn.0.bias', 'embedder.model.transformer.blocks.12.ffn.1.weight', 'embedder.model.transformer.blocks.24.attn.k_ln.weight', 'embedder.model.transformer.blocks.18.ffn.1.weight', 'embedder.model.transformer.blocks.27.ffn.0.weight', 'embedder.model.transformer.blocks.7.attn.q_ln.weight', 'embedder.model.transformer.blocks.19.attn.k_ln.weight', 'embedder.model.transformer.blocks.22.attn.out_proj.weight', 'embedder.model.transformer.blocks.21.attn.out_proj.weight', 'embedder.model.transformer.blocks.15.attn.layernorm_qkv.1.weight', 'embedder.model.transformer.blocks.6.attn.layernorm_qkv.0.bias', 'embedder.model.transformer.blocks.19.ffn.0.weight', 'embedder.model.transformer.blocks.21.attn.layernorm_qkv.0.weight', 'embedder.model.transformer.blocks.22.ffn.3.weight', 'embedder.model.transformer.blocks.26.ffn.0.weight', 'embedder.model.transformer.blocks.20.ffn.0.bias', 'embedder.model.transformer.blocks.4.attn.layernorm_qkv.0.weight', 'embedder.model.transformer.blocks.1.attn.k_ln.weight', 'embedder.model.transformer.blocks.1.attn.layernorm_qkv.1.weight', 'embedder.model.transformer.blocks.25.ffn.1.weight', 'embedder.model.transformer.blocks.24.attn.q_ln.weight', 'embedder.model.transformer.blocks.11.attn.q_ln.weight', 'embedder.model.transformer.blocks.0.attn.layernorm_qkv.1.weight', 'embedder.model.transformer.blocks.15.attn.k_ln.weight', 'embedder.model.transformer.blocks.26.ffn.3.weight', 'embedder.model.transformer.blocks.29.attn.k_ln.weight', 'embedder.model.transformer.blocks.14.attn.q_ln.weight', 'embedder.model.transformer.blocks.7.attn.k_ln.weight', 'embedder.model.transformer.blocks.7.attn.layernorm_qkv.0.weight', 'embedder.model.transformer.blocks.5.ffn.3.weight', 'embedder.model.transformer.blocks.6.attn.layernorm_qkv.0.weight', 'embedder.model.transformer.blocks.11.attn.k_ln.weight', 'embedder.model.transformer.blocks.7.ffn.0.weight', 'embedder.model.transformer.blocks.20.ffn.1.weight', 'embedder.model.transformer.blocks.22.attn.layernorm_qkv.1.weight', 'embedder.model.transformer.blocks.29.attn.layernorm_qkv.0.bias', 'embedder.model.transformer.blocks.11.attn.layernorm_qkv.0.weight', 'embedder.model.transformer.blocks.23.attn.q_ln.weight', 'embedder.model.transformer.blocks.9.attn.layernorm_qkv.0.bias', 'embedder.model.transformer.blocks.17.attn.layernorm_qkv.0.bias', 'embedder.model.transformer.blocks.10.attn.layernorm_qkv.0.weight', 'embedder.model.transformer.blocks.23.attn.k_ln.weight', 'embedder.model.transformer.blocks.0.ffn.0.weight', 'embedder.model.transformer.blocks.23.ffn.0.weight', 'embedder.model.transformer.blocks.8.ffn.3.weight', 'embedder.model.transformer.blocks.3.attn.out_proj.weight', 'embedder.model.transformer.blocks.23.attn.out_proj.weight', 'embedder.model.transformer.blocks.23.ffn.0.bias', 'embedder.model.transformer.blocks.25.ffn.3.weight', 'embedder.model.transformer.blocks.27.attn.k_ln.weight', 'embedder.model.transformer.blocks.18.attn.out_proj.weight', 'embedder.model.transformer.blocks.5.attn.out_proj.weight', 'embedder.model.transformer.blocks.4.ffn.0.bias', 'embedder.model.transformer.blocks.2.ffn.0.bias', 'embedder.model.transformer.blocks.10.attn.layernorm_qkv.1.weight', 'embedder.model.transformer.blocks.28.attn.layernorm_qkv.0.bias', 'embedder.model.transformer.blocks.28.attn.layernorm_qkv.1.weight', 'embedder.model.transformer.blocks.2.attn.q_ln.weight', 'embedder.model.transformer.blocks.21.ffn.3.weight', 'embedder.model.transformer.blocks.13.ffn.0.bias', 'embedder.model.transformer.blocks.8.ffn.0.weight', 'embedder.model.transformer.norm.weight', 'embedder.model.transformer.blocks.19.attn.layernorm_qkv.0.bias', 'embedder.model.transformer.blocks.29.ffn.1.weight', 'embedder.model.transformer.blocks.17.attn.layernorm_qkv.0.weight', 'embedder.model.transformer.blocks.9.ffn.1.weight', 'embedder.model.transformer.blocks.15.attn.out_proj.weight', 'embedder.model.transformer.blocks.16.attn.layernorm_qkv.0.weight', 'embedder.model.transformer.blocks.24.ffn.1.weight', 'embedder.model.transformer.blocks.26.attn.out_proj.weight', 'embedder.model.transformer.blocks.2.attn.layernorm_qkv.0.weight', 'embedder.model.transformer.blocks.3.ffn.3.weight', 'embedder.model.transformer.blocks.13.attn.layernorm_qkv.1.weight', 'embedder.model.transformer.blocks.13.attn.out_proj.weight', 'embedder.model.transformer.blocks.24.attn.out_proj.weight', 'embedder.model.transformer.blocks.15.ffn.3.weight', 'embedder.model.transformer.blocks.12.attn.q_ln.weight', 'embedder.model.transformer.blocks.8.attn.layernorm_qkv.0.bias', 'embedder.model.transformer.blocks.4.ffn.0.weight', 'embedder.model.transformer.blocks.13.attn.layernorm_qkv.0.weight', 'embedder.model.transformer.blocks.27.attn.layernorm_qkv.0.weight', 'embedder.model.transformer.blocks.3.ffn.1.weight', 'embedder.model.transformer.blocks.8.ffn.0.bias', 'embedder.model.transformer.blocks.9.attn.layernorm_qkv.0.weight', 'embedder.model.transformer.blocks.17.attn.out_proj.weight', 'embedder.model.transformer.blocks.18.ffn.0.bias', 'embedder.model.transformer.blocks.10.attn.out_proj.weight', 'embedder.model.transformer.blocks.1.attn.layernorm_qkv.0.weight', 'embedder.model.sequence_head.0.weight', 'embedder.model.transformer.blocks.9.ffn.0.bias', 'embedder.model.transformer.blocks.25.ffn.0.weight', 'embedder.model.transformer.blocks.0.attn.k_ln.weight', 'embedder.model.transformer.blocks.11.attn.layernorm_qkv.1.weight', 'embedder.model.transformer.blocks.0.attn.out_proj.weight', 'embedder.model.transformer.blocks.14.attn.k_ln.weight', 'embedder.model.transformer.blocks.5.attn.k_ln.weight', 'embedder.model.transformer.blocks.12.attn.layernorm_qkv.1.weight', 'embedder.model.transformer.blocks.25.attn.layernorm_qkv.1.weight', 'embedder.model.transformer.blocks.24.attn.layernorm_qkv.0.weight', 'embedder.model.transformer.blocks.0.attn.layernorm_qkv.0.bias', 'embedder.model.transformer.blocks.26.attn.k_ln.weight', 'embedder.model.transformer.blocks.10.attn.q_ln.weight', 'embedder.model.transformer.blocks.28.ffn.0.weight', 'embedder.model.transformer.blocks.17.ffn.0.weight', 'embedder.model.transformer.blocks.12.attn.k_ln.weight', 'embedder.model.transformer.blocks.21.attn.layernorm_qkv.1.weight', 'embedder.model.transformer.blocks.29.ffn.3.weight', 'embedder.model.transformer.blocks.1.attn.layernorm_qkv.0.bias', 'embedder.model.transformer.blocks.7.ffn.1.weight', 'embedder.model.transformer.blocks.14.attn.layernorm_qkv.0.bias', 'embedder.model.transformer.blocks.25.attn.k_ln.weight', 'embedder.model.transformer.blocks.1.attn.out_proj.weight', 'embedder.model.transformer.blocks.2.attn.layernorm_qkv.1.weight', 'embedder.model.transformer.blocks.8.attn.layernorm_qkv.1.weight', 'embedder.model.transformer.blocks.11.attn.layernorm_qkv.0.bias', 'embedder.model.transformer.blocks.18.attn.layernorm_qkv.1.weight', 'embedder.model.transformer.blocks.26.ffn.1.weight', 'embedder.model.transformer.blocks.3.attn.q_ln.weight', 'embedder.model.transformer.blocks.28.attn.layernorm_qkv.0.weight', 'embedder.model.transformer.blocks.6.attn.q_ln.weight', 'embedder.model.transformer.blocks.26.attn.q_ln.weight', 'embedder.model.transformer.blocks.1.ffn.0.bias', 'embedder.model.transformer.blocks.29.ffn.0.weight', 'embedder.model.transformer.blocks.0.ffn.3.weight', 'embedder.model.transformer.blocks.18.attn.layernorm_qkv.0.weight', 'embedder.model.transformer.blocks.23.ffn.1.weight', 'embedder.model.transformer.blocks.16.ffn.0.weight', 'embedder.model.transformer.blocks.17.attn.k_ln.weight', 'embedder.model.transformer.blocks.13.ffn.1.weight', 'embedder.model.transformer.blocks.0.attn.layernorm_qkv.0.weight', 'embedder.model.transformer.blocks.23.ffn.3.weight', 'embedder.model.transformer.blocks.13.attn.layernorm_qkv.0.bias', 'embedder.model.transformer.blocks.20.attn.k_ln.weight', 'embedder.model.transformer.blocks.11.ffn.0.weight', 'embedder.model.transformer.blocks.3.attn.k_ln.weight', 'embedder.model.transformer.blocks.23.attn.layernorm_qkv.0.bias', 'embedder.model.transformer.blocks.26.attn.layernorm_qkv.0.bias', 'embedder.model.transformer.blocks.15.attn.layernorm_qkv.0.weight', 'embedder.model.transformer.blocks.25.attn.out_proj.weight', 'embedder.model.transformer.blocks.23.attn.layernorm_qkv.1.weight', 'embedder.model.transformer.blocks.10.ffn.1.weight', 'embedder.model.transformer.blocks.24.attn.layernorm_qkv.1.weight', 'embedder.model.transformer.blocks.11.ffn.3.weight', 'embedder.model.transformer.blocks.15.ffn.0.weight', 'embedder.model.transformer.blocks.3.attn.layernorm_qkv.0.weight', 'embedder.model.transformer.blocks.7.attn.layernorm_qkv.0.bias', 'embedder.model.transformer.blocks.16.attn.layernorm_qkv.1.weight', 'embedder.model.transformer.blocks.17.attn.layernorm_qkv.1.weight', 'embedder.model.transformer.blocks.6.attn.layernorm_qkv.1.weight', 'embedder.model.transformer.blocks.6.attn.k_ln.weight', 'embedder.model.transformer.blocks.5.attn.layernorm_qkv.0.weight', 'embedder.model.transformer.blocks.22.ffn.1.weight', 'embedder.model.transformer.blocks.19.attn.q_ln.weight', 'embedder.model.transformer.blocks.14.ffn.3.weight', 'embedder.model.transformer.blocks.25.ffn.0.bias', 'embedder.model.transformer.blocks.19.ffn.0.bias', 'embedder.model.transformer.blocks.10.attn.layernorm_qkv.0.bias', 'embedder.model.transformer.blocks.2.ffn.1.weight', 'embedder.model.transformer.blocks.12.attn.out_proj.weight', 'embedder.model.transformer.blocks.0.ffn.1.weight', 'embedder.model.transformer.blocks.21.attn.q_ln.weight', 'embedder.model.transformer.blocks.26.attn.layernorm_qkv.1.weight', 'embedder.model.transformer.blocks.18.ffn.0.weight', 'embedder.model.transformer.blocks.29.attn.layernorm_qkv.1.weight'}\n",
      "Base model has 308 parameters\n",
      "Checkpoint has 308 parameters\n",
      "Missing in checkpoint (308):\n",
      "  transformer.blocks.12.attn.k_ln.weight\n",
      "  transformer.blocks.14.ffn.0.bias\n",
      "  transformer.blocks.22.attn.k_ln.weight\n",
      "  transformer.blocks.16.attn.layernorm_qkv.0.bias\n",
      "  transformer.blocks.11.ffn.3.weight\n",
      "Extra in checkpoint (308):\n",
      "  embedder.model.transformer.blocks.29.attn.layernorm_qkv.0.weight\n",
      "  embedder.model.transformer.blocks.5.ffn.0.weight\n",
      "  embedder.model.transformer.blocks.5.ffn.1.weight\n",
      "  embedder.model.transformer.blocks.4.attn.q_ln.weight\n",
      "  embedder.model.transformer.blocks.28.attn.q_ln.weight\n"
     ]
    }
   ],
   "source": [
    "base_model = esmc\n",
    "\n",
    "base_keys = set(base_model.state_dict().keys())\n",
    "print(base_keys)\n",
    "checkpoint_keys = set(new_state_dict.keys())\n",
    "print(checkpoint_keys)\n",
    "\n",
    "print(f\"Base model has {len(base_keys)} parameters\")\n",
    "print(f\"Checkpoint has {len(checkpoint_keys)} parameters\")\n",
    "\n",
    "# Find mismatched keys\n",
    "missing_in_checkpoint = base_keys - checkpoint_keys\n",
    "extra_in_checkpoint = checkpoint_keys - base_keys\n",
    "\n",
    "if missing_in_checkpoint:\n",
    "    print(f\"Missing in checkpoint ({len(missing_in_checkpoint)}):\")\n",
    "    for key in list(missing_in_checkpoint)[:5]:  # Show first 5\n",
    "        print(f\"  {key}\")\n",
    "\n",
    "if extra_in_checkpoint:\n",
    "    print(f\"Extra in checkpoint ({len(extra_in_checkpoint)}):\")\n",
    "    for key in list(extra_in_checkpoint)[:5]:  # Show first 5\n",
    "        print(f\"  {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove prefix from checkpoint keys\n",
    "prefix_state_dict = {}\n",
    "prefix_to_remove = \"embedder.model.\"  # adjust as needed\n",
    "for key, value in new_state_dict.items():\n",
    "    if key.startswith(prefix_to_remove):\n",
    "        new_key = key[len(prefix_to_remove):]\n",
    "        prefix_state_dict[new_key] = value\n",
    "    else:\n",
    "        prefix_state_dict[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'transformer.blocks.12.attn.k_ln.weight', 'transformer.blocks.14.ffn.0.bias', 'transformer.blocks.22.attn.k_ln.weight', 'transformer.blocks.16.attn.layernorm_qkv.0.bias', 'transformer.blocks.11.ffn.3.weight', 'transformer.blocks.9.ffn.3.weight', 'transformer.blocks.11.attn.q_ln.weight', 'transformer.blocks.26.ffn.0.bias', 'transformer.blocks.8.ffn.3.weight', 'transformer.blocks.23.attn.k_ln.weight', 'transformer.blocks.18.attn.out_proj.weight', 'transformer.blocks.15.attn.k_ln.weight', 'transformer.blocks.27.ffn.0.weight', 'transformer.blocks.29.ffn.3.weight', 'sequence_head.0.weight', 'transformer.blocks.1.attn.layernorm_qkv.0.bias', 'transformer.blocks.5.attn.out_proj.weight', 'transformer.blocks.3.attn.layernorm_qkv.1.weight', 'transformer.blocks.21.attn.layernorm_qkv.0.weight', 'transformer.blocks.26.ffn.1.weight', 'transformer.blocks.9.attn.q_ln.weight', 'transformer.blocks.18.attn.layernorm_qkv.0.weight', 'transformer.blocks.18.attn.q_ln.weight', 'transformer.blocks.25.attn.layernorm_qkv.0.weight', 'transformer.blocks.12.attn.layernorm_qkv.0.bias', 'transformer.blocks.27.attn.q_ln.weight', 'transformer.blocks.26.attn.layernorm_qkv.1.weight', 'transformer.blocks.5.ffn.3.weight', 'transformer.blocks.16.ffn.0.bias', 'transformer.blocks.20.attn.layernorm_qkv.1.weight', 'transformer.blocks.16.attn.layernorm_qkv.1.weight', 'transformer.blocks.20.ffn.1.weight', 'transformer.blocks.24.ffn.3.weight', 'transformer.blocks.24.ffn.0.bias', 'transformer.blocks.10.attn.out_proj.weight', 'transformer.blocks.2.ffn.1.weight', 'transformer.blocks.14.attn.layernorm_qkv.1.weight', 'transformer.blocks.28.ffn.0.weight', 'transformer.blocks.10.attn.layernorm_qkv.0.weight', 'transformer.blocks.7.ffn.0.bias', 'transformer.blocks.9.ffn.1.weight', 'transformer.blocks.17.ffn.0.weight', 'transformer.blocks.13.attn.layernorm_qkv.0.weight', 'transformer.blocks.13.attn.layernorm_qkv.0.bias', 'transformer.blocks.11.attn.layernorm_qkv.1.weight', 'transformer.blocks.9.ffn.0.bias', 'transformer.blocks.18.ffn.1.weight', 'transformer.blocks.22.attn.layernorm_qkv.0.weight', 'transformer.blocks.8.ffn.1.weight', 'transformer.blocks.10.ffn.1.weight', 'transformer.blocks.9.attn.layernorm_qkv.0.weight', 'transformer.blocks.21.ffn.1.weight', 'transformer.blocks.7.attn.k_ln.weight', 'transformer.blocks.17.attn.layernorm_qkv.0.weight', 'transformer.blocks.10.ffn.3.weight', 'transformer.blocks.1.attn.k_ln.weight', 'transformer.blocks.21.attn.layernorm_qkv.1.weight', 'transformer.blocks.27.attn.out_proj.weight', 'transformer.blocks.9.attn.out_proj.weight', 'transformer.blocks.16.ffn.3.weight', 'transformer.blocks.5.ffn.1.weight', 'transformer.blocks.6.ffn.1.weight', 'transformer.blocks.14.attn.layernorm_qkv.0.bias', 'transformer.blocks.12.attn.q_ln.weight', 'transformer.blocks.2.attn.out_proj.weight', 'transformer.blocks.22.attn.q_ln.weight', 'transformer.blocks.8.ffn.0.bias', 'sequence_head.0.bias', 'transformer.blocks.23.ffn.3.weight', 'transformer.blocks.24.attn.k_ln.weight', 'sequence_head.2.bias', 'transformer.blocks.17.attn.layernorm_qkv.0.bias', 'transformer.blocks.28.attn.out_proj.weight', 'transformer.blocks.3.attn.out_proj.weight', 'transformer.blocks.15.attn.out_proj.weight', 'transformer.blocks.6.attn.layernorm_qkv.0.weight', 'transformer.blocks.29.ffn.1.weight', 'transformer.blocks.5.attn.layernorm_qkv.1.weight', 'transformer.blocks.19.ffn.3.weight', 'transformer.blocks.25.attn.k_ln.weight', 'transformer.blocks.2.attn.layernorm_qkv.0.weight', 'transformer.blocks.1.attn.layernorm_qkv.1.weight', 'transformer.blocks.24.attn.layernorm_qkv.1.weight', 'transformer.blocks.0.ffn.0.bias', 'transformer.blocks.16.attn.q_ln.weight', 'transformer.blocks.17.attn.q_ln.weight', 'transformer.blocks.21.attn.out_proj.weight', 'transformer.blocks.29.attn.out_proj.weight', 'transformer.blocks.23.ffn.0.bias', 'transformer.blocks.29.attn.layernorm_qkv.0.weight', 'transformer.blocks.18.ffn.0.bias', 'transformer.blocks.4.attn.out_proj.weight', 'transformer.blocks.12.ffn.1.weight', 'transformer.blocks.3.ffn.0.bias', 'transformer.blocks.16.ffn.0.weight', 'transformer.blocks.27.attn.k_ln.weight', 'transformer.blocks.4.attn.layernorm_qkv.0.bias', 'sequence_head.3.bias', 'transformer.blocks.7.attn.layernorm_qkv.0.weight', 'transformer.blocks.14.ffn.0.weight', 'transformer.blocks.2.ffn.3.weight', 'transformer.blocks.3.ffn.1.weight', 'transformer.blocks.22.attn.layernorm_qkv.0.bias', 'transformer.blocks.19.attn.layernorm_qkv.0.bias', 'transformer.blocks.0.attn.layernorm_qkv.1.weight', 'transformer.blocks.2.attn.layernorm_qkv.0.bias', 'transformer.blocks.17.attn.layernorm_qkv.1.weight', 'transformer.blocks.2.ffn.0.bias', 'transformer.blocks.8.attn.k_ln.weight', 'transformer.blocks.16.attn.k_ln.weight', 'transformer.blocks.29.attn.layernorm_qkv.1.weight', 'transformer.blocks.1.attn.layernorm_qkv.0.weight', 'transformer.blocks.9.attn.layernorm_qkv.0.bias', 'transformer.blocks.12.ffn.0.bias', 'transformer.blocks.4.ffn.0.bias', 'transformer.blocks.20.attn.k_ln.weight', 'transformer.blocks.23.attn.out_proj.weight', 'transformer.blocks.23.attn.q_ln.weight', 'transformer.blocks.22.ffn.1.weight', 'transformer.blocks.13.attn.out_proj.weight', 'transformer.blocks.21.attn.q_ln.weight', 'transformer.blocks.25.attn.layernorm_qkv.1.weight', 'transformer.blocks.11.attn.out_proj.weight', 'transformer.blocks.13.attn.q_ln.weight', 'transformer.blocks.3.ffn.0.weight', 'transformer.blocks.5.attn.k_ln.weight', 'transformer.blocks.18.attn.k_ln.weight', 'transformer.blocks.12.attn.layernorm_qkv.0.weight', 'transformer.blocks.5.attn.q_ln.weight', 'transformer.blocks.15.attn.layernorm_qkv.1.weight', 'transformer.blocks.14.attn.q_ln.weight', 'transformer.blocks.19.ffn.1.weight', 'transformer.blocks.19.ffn.0.weight', 'transformer.blocks.0.ffn.1.weight', 'transformer.blocks.0.attn.layernorm_qkv.0.bias', 'sequence_head.2.weight', 'transformer.blocks.15.attn.layernorm_qkv.0.weight', 'transformer.blocks.6.attn.layernorm_qkv.0.bias', 'transformer.blocks.17.ffn.3.weight', 'transformer.blocks.0.ffn.3.weight', 'transformer.blocks.2.attn.q_ln.weight', 'transformer.blocks.26.attn.layernorm_qkv.0.bias', 'transformer.blocks.27.attn.layernorm_qkv.0.weight', 'transformer.blocks.15.ffn.3.weight', 'transformer.blocks.19.attn.layernorm_qkv.1.weight', 'transformer.blocks.12.attn.layernorm_qkv.1.weight', 'transformer.blocks.28.ffn.1.weight', 'transformer.blocks.22.ffn.0.weight', 'transformer.blocks.27.ffn.0.bias', 'transformer.blocks.14.ffn.1.weight', 'transformer.blocks.10.ffn.0.bias', 'transformer.blocks.23.attn.layernorm_qkv.1.weight', 'transformer.blocks.1.attn.q_ln.weight', 'transformer.blocks.10.attn.q_ln.weight', 'transformer.blocks.20.ffn.3.weight', 'transformer.blocks.3.attn.layernorm_qkv.0.weight', 'transformer.blocks.27.attn.layernorm_qkv.0.bias', 'transformer.blocks.22.attn.out_proj.weight', 'transformer.blocks.19.attn.layernorm_qkv.0.weight', 'transformer.blocks.15.attn.layernorm_qkv.0.bias', 'transformer.blocks.13.ffn.0.bias', 'transformer.blocks.16.ffn.1.weight', 'transformer.blocks.20.attn.layernorm_qkv.0.weight', 'transformer.blocks.11.ffn.1.weight', 'transformer.blocks.26.ffn.3.weight', 'transformer.blocks.5.ffn.0.bias', 'transformer.blocks.1.ffn.0.bias', 'transformer.blocks.6.attn.q_ln.weight', 'transformer.blocks.4.ffn.1.weight', 'transformer.blocks.21.ffn.0.bias', 'transformer.blocks.28.ffn.0.bias', 'transformer.blocks.21.attn.layernorm_qkv.0.bias', 'transformer.blocks.17.attn.k_ln.weight', 'transformer.blocks.7.ffn.3.weight', 'transformer.blocks.8.attn.layernorm_qkv.1.weight', 'transformer.blocks.18.ffn.0.weight', 'transformer.blocks.25.ffn.0.weight', 'transformer.blocks.6.attn.k_ln.weight', 'transformer.blocks.28.attn.layernorm_qkv.1.weight', 'transformer.blocks.17.attn.out_proj.weight', 'transformer.blocks.23.attn.layernorm_qkv.0.weight', 'transformer.blocks.25.attn.layernorm_qkv.0.bias', 'transformer.blocks.10.ffn.0.weight', 'transformer.blocks.14.attn.layernorm_qkv.0.weight', 'transformer.blocks.26.attn.q_ln.weight', 'transformer.blocks.13.ffn.1.weight', 'transformer.blocks.5.attn.layernorm_qkv.0.weight', 'transformer.blocks.4.ffn.0.weight', 'transformer.blocks.28.attn.layernorm_qkv.0.weight', 'transformer.blocks.3.attn.layernorm_qkv.0.bias', 'transformer.blocks.0.ffn.0.weight', 'transformer.blocks.19.attn.q_ln.weight', 'transformer.blocks.25.attn.q_ln.weight', 'transformer.blocks.24.ffn.1.weight', 'embed.weight', 'transformer.blocks.11.ffn.0.weight', 'transformer.blocks.2.attn.k_ln.weight', 'transformer.blocks.19.ffn.0.bias', 'transformer.blocks.7.ffn.0.weight', 'transformer.blocks.15.ffn.1.weight', 'transformer.blocks.11.ffn.0.bias', 'transformer.blocks.4.ffn.3.weight', 'transformer.blocks.26.ffn.0.weight', 'transformer.blocks.24.attn.layernorm_qkv.0.weight', 'transformer.blocks.9.attn.k_ln.weight', 'transformer.blocks.20.ffn.0.bias', 'transformer.blocks.13.ffn.3.weight', 'transformer.blocks.16.attn.layernorm_qkv.0.weight', 'transformer.blocks.24.attn.out_proj.weight', 'transformer.blocks.12.ffn.3.weight', 'transformer.blocks.0.attn.q_ln.weight', 'transformer.blocks.16.attn.out_proj.weight', 'transformer.blocks.15.ffn.0.bias', 'transformer.blocks.28.attn.q_ln.weight', 'transformer.blocks.4.attn.q_ln.weight', 'transformer.blocks.6.attn.out_proj.weight', 'transformer.blocks.18.attn.layernorm_qkv.1.weight', 'transformer.blocks.5.ffn.0.weight', 'transformer.blocks.18.ffn.3.weight', 'transformer.blocks.3.attn.k_ln.weight', 'sequence_head.3.weight', 'transformer.blocks.19.attn.k_ln.weight', 'transformer.blocks.22.attn.layernorm_qkv.1.weight', 'transformer.blocks.22.ffn.3.weight', 'transformer.blocks.25.ffn.3.weight', 'transformer.blocks.7.attn.out_proj.weight', 'transformer.blocks.29.ffn.0.weight', 'transformer.blocks.9.attn.layernorm_qkv.1.weight', 'transformer.blocks.1.attn.out_proj.weight', 'transformer.blocks.6.ffn.3.weight', 'transformer.blocks.24.attn.layernorm_qkv.0.bias', 'transformer.blocks.27.ffn.3.weight', 'transformer.blocks.24.attn.q_ln.weight', 'transformer.blocks.20.ffn.0.weight', 'transformer.blocks.0.attn.k_ln.weight', 'transformer.blocks.4.attn.k_ln.weight', 'transformer.blocks.6.attn.layernorm_qkv.1.weight', 'transformer.blocks.23.attn.layernorm_qkv.0.bias', 'transformer.blocks.28.ffn.3.weight', 'transformer.blocks.4.attn.layernorm_qkv.1.weight', 'transformer.blocks.20.attn.q_ln.weight', 'transformer.blocks.7.ffn.1.weight', 'transformer.blocks.13.attn.layernorm_qkv.1.weight', 'transformer.blocks.17.ffn.1.weight', 'transformer.blocks.29.ffn.0.bias', 'transformer.norm.weight', 'transformer.blocks.28.attn.k_ln.weight', 'transformer.blocks.14.attn.out_proj.weight', 'transformer.blocks.3.ffn.3.weight', 'transformer.blocks.10.attn.k_ln.weight', 'transformer.blocks.7.attn.q_ln.weight', 'transformer.blocks.10.attn.layernorm_qkv.1.weight', 'transformer.blocks.13.attn.k_ln.weight', 'transformer.blocks.14.ffn.3.weight', 'transformer.blocks.8.ffn.0.weight', 'transformer.blocks.7.attn.layernorm_qkv.1.weight', 'transformer.blocks.18.attn.layernorm_qkv.0.bias', 'transformer.blocks.11.attn.layernorm_qkv.0.weight', 'transformer.blocks.12.attn.out_proj.weight', 'transformer.blocks.26.attn.k_ln.weight', 'transformer.blocks.7.attn.layernorm_qkv.0.bias', 'transformer.blocks.9.ffn.0.weight', 'transformer.blocks.4.attn.layernorm_qkv.0.weight', 'transformer.blocks.1.ffn.3.weight', 'transformer.blocks.23.ffn.1.weight', 'transformer.blocks.17.ffn.0.bias', 'transformer.blocks.29.attn.k_ln.weight', 'transformer.blocks.5.attn.layernorm_qkv.0.bias', 'transformer.blocks.27.ffn.1.weight', 'transformer.blocks.24.ffn.0.weight', 'transformer.blocks.13.ffn.0.weight', 'transformer.blocks.15.attn.q_ln.weight', 'transformer.blocks.10.attn.layernorm_qkv.0.bias', 'transformer.blocks.12.ffn.0.weight', 'transformer.blocks.15.ffn.0.weight', 'transformer.blocks.6.ffn.0.bias', 'transformer.blocks.2.ffn.0.weight', 'transformer.blocks.22.ffn.0.bias', 'transformer.blocks.27.attn.layernorm_qkv.1.weight', 'transformer.blocks.21.ffn.0.weight', 'transformer.blocks.11.attn.layernorm_qkv.0.bias', 'transformer.blocks.26.attn.out_proj.weight', 'transformer.blocks.3.attn.q_ln.weight', 'transformer.blocks.21.ffn.3.weight', 'transformer.blocks.28.attn.layernorm_qkv.0.bias', 'transformer.blocks.8.attn.q_ln.weight', 'transformer.blocks.20.attn.layernorm_qkv.0.bias', 'transformer.blocks.25.ffn.1.weight', 'transformer.blocks.29.attn.layernorm_qkv.0.bias', 'transformer.blocks.8.attn.layernorm_qkv.0.bias', 'transformer.blocks.14.attn.k_ln.weight', 'transformer.blocks.2.attn.layernorm_qkv.1.weight', 'transformer.blocks.1.ffn.1.weight', 'transformer.blocks.0.attn.out_proj.weight', 'transformer.blocks.1.ffn.0.weight', 'transformer.blocks.23.ffn.0.weight', 'transformer.blocks.25.attn.out_proj.weight', 'transformer.blocks.8.attn.out_proj.weight', 'transformer.blocks.29.attn.q_ln.weight', 'transformer.blocks.6.ffn.0.weight', 'transformer.blocks.25.ffn.0.bias', 'transformer.blocks.20.attn.out_proj.weight', 'transformer.blocks.8.attn.layernorm_qkv.0.weight', 'transformer.blocks.21.attn.k_ln.weight', 'transformer.blocks.0.attn.layernorm_qkv.0.weight', 'transformer.blocks.26.attn.layernorm_qkv.0.weight', 'transformer.blocks.19.attn.out_proj.weight', 'transformer.blocks.11.attn.k_ln.weight'}\n"
     ]
    }
   ],
   "source": [
    "prefix_keys = set(prefix_state_dict.keys())\n",
    "print(prefix_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find mismatched keys\n",
    "missing_in_checkpoint = base_keys - prefix_keys\n",
    "extra_in_checkpoint = prefix_keys - base_keys\n",
    "\n",
    "if missing_in_checkpoint:\n",
    "    print(f\"Missing in checkpoint ({len(missing_in_checkpoint)}):\")\n",
    "    for key in list(missing_in_checkpoint)[:5]:  # Show first 5\n",
    "        print(f\"  {key}\")\n",
    "\n",
    "if extra_in_checkpoint:\n",
    "    print(f\"Extra in checkpoint ({len(extra_in_checkpoint)}):\")\n",
    "    for key in list(extra_in_checkpoint)[:5]:  # Show first 5\n",
    "        print(f\"  {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.load_state_dict(prefix_state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original model had 317 parameters\n",
      "New model has 308 parameters\n",
      "Removed 9 MLP parameters\n",
      "Saved to: /net/vast-storage/scratch/vast/kellislab/artliang/magneton/finetuning/esmc_300m_ss_ewc/checkpoints_esmc_300m_ss_ewc/model_esmc_300m_ss_lr_default_no_mlp.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Create new model with the filtered state_dict\n",
    "new_model = model.copy()  # Keep other metadata\n",
    "new_model['state_dict'] = prefix_state_dict\n",
    "\n",
    "# Save the new model\n",
    "# new_ckpt_path = '/weka/scratch/weka/kellislab/rcalef/projects/magneton/experiments/downstream_evals/deepfri_esmc_300m_domain_default_lr/EC/epoch=0-valid_auprc=0.02_no_mlp.ckpt'\n",
    "new_ckpt_path = f'{ckpt_path}_no_mlp.ckpt'\n",
    "torch.save(new_model, new_ckpt_path)\n",
    "\n",
    "print(f\"\\nOriginal model had {len(model['state_dict'])} parameters\")\n",
    "print(f\"New model has {len(prefix_state_dict)} parameters\")\n",
    "print(f\"Removed {len(model['state_dict']) - len(prefix_state_dict)} MLP parameters\")\n",
    "print(f\"Saved to: {new_ckpt_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (magneton)",
   "language": "python",
   "name": "magneton"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
