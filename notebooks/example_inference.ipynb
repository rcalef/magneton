{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",ESMCBaseModel
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from magneton.config import (\n",
    "    DataConfig,\n",
    "    EmbeddingConfig,\n",
    "    EvalConfig,\n",
    "    ModelConfig,\n",
    "    TrainingConfig,\n",
    "    PipelineConfig,\n",
    ")\n",
    "from magneton.embedders.esmc_embedder import ESMCConfig, ESMCDataModule, ESMCEmbedder\n",
    "from magneton.evals.supervised_classification import MultiLabelMLP, run_supervised_classification\n",
    "from magneton.training.embedding_mlp import EmbeddingMLP, MultitaskEmbeddingMLP\n",
    "from magneton.types import InterProType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from magneton.data.supervised_dataset import DeepFRIDataConfig, DeepFRIDataModule\n",
    "from esm.tokenization import get_esmc_model_tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"EC\"\n",
    "config = DeepFRIDataConfig(\n",
    "    data_dir=\"/weka/scratch/weka/kellislab/rcalef/data/magneton-data/evaluations\",\n",
    "    task=task,\n",
    ")\n",
    "\n",
    "tokenizer = get_esmc_model_tokenizers()\n",
    "module = DeepFRIDataModule(\n",
    "    config=config,\n",
    "    seq_tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = module.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SupervisedBatch(protein_ids=['3TD7-A', '4XI1-A', '5VGR-A', '2WTZ-A', '4IZJ-B', '1DLI-A', '4WZ0-A', '4BMV-A', '3B9G-A', '1Q3I-A', '2XYR-A', '2BA0-F', '6BI4-A', '2FBN-A', '2UYK-A', '2BE3-A', '3RST-A', '5LV0-A', '2X8T-A', '4FMV-A', '4LEC-A', '5G3P-A', '5LOU-A', '4JGU-A', '1CQD-A', '2XBZ-A', '2PDO-A', '1TON-A', '2NN6-G', '1A47-A', '1BHE-A', '3TDE-A'], labels=tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), tokenized_seq=tensor([[ 0,  6,  8,  ...,  1,  1,  1],\n",
       "        [ 0, 17, 19,  ...,  1,  1,  1],\n",
       "        [ 0,  8,  8,  ...,  1,  1,  1],\n",
       "        ...,\n",
       "        [ 0,  5,  8,  ...,  1,  1,  1],\n",
       "        [ 0,  8, 13,  ...,  1,  1,  1],\n",
       "        [ 0,  6, 14,  ...,  1,  1,  1]]), structure_paths=['NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_config = EvalConfig(\n",
    "    tasks=[\"GO:MF\"],\n",
    "    data_dir=config.data_dir,\n",
    "    #model_checkpoint=\"/weka/scratch/weka/kellislab/rcalef/projects/magneton/experiments/no_finetune/esmc_300m_domain/model_esmc_300m_domain.pt\",\n",
    "    model_checkpoint=\"/weka/scratch/weka/kellislab/rcalef/projects/magneton/experiments/esmc_test/model_ewc_domain.pt\",\n",
    "    model_params={\n",
    "        \"batch_size\": 32,\n",
    "        \"hidden_dims\": [256, 256, 256],\n",
    "        \"dropout_rate\": 0.1,\n",
    "        \"learning_rate\": 1e-2,\n",
    "        \"weight_decay\": 0.0,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Creating esmc_300m embedder ===\n",
      "EmbeddingConfig(_target_='magneton.config.EmbeddingConfig',\n",
      "                model='esmc_300m',\n",
      "                model_params={'model_size': '300m', 'weights_path': '/weka/scratch/weka/kellislab/rcalef/model_weights/esmc-300m-2024-12', 'rep_layer': 29, 'use_flash_attn': True})\n",
      "Config parameters: None\n",
      "MLP Device: cpu\n"
     ]
    }
   ],
   "source": [
    "model = EmbeddingMLP.load_from_checkpoint(\n",
    "    eval_config.model_checkpoint,\n",
    "    load_pretrained_fisher=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Creating esmc_300m embedder ===\n",
      "EmbeddingConfig(_target_='magneton.config.EmbeddingConfig',\n",
      "                model='esmc_300m',\n",
      "                model_params={'model_size': '300m', 'weights_path': '/weka/scratch/weka/kellislab/rcalef/model_weights/esmc-300m-2024-12', 'rep_layer': 29, 'use_flash_attn': False})\n",
      "Config parameters: None\n",
      "MLP Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Note, this is a somewhat frustrating workaround needed for now\n",
    "# to load models that were trained using Flash Attention if you're\n",
    "# now running inference on a machine with a GPU that doesn't support\n",
    "# Flash Attention (i.e. non-A100 on OpenMind)\n",
    "new_embed_config = model.embed_config\n",
    "new_embed_config.model_params[\"use_flash_attn\"] = False\n",
    "\n",
    "new_config = PipelineConfig(\n",
    "    model=model.model_config,\n",
    "    training=model.train_config,\n",
    "    embedding=new_embed_config,\n",
    ")\n",
    "new_config\n",
    "\n",
    "model = EmbeddingMLP.load_from_checkpoint(\n",
    "    eval_config.model_checkpoint,\n",
    "    config=new_config,\n",
    "    load_pretrained_fisher=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbeddingMLP(\n",
       "  (embedder): ESMCEmbedder(\n",
       "    (model): ESMC(\n",
       "      (embed): Embedding(64, 960)\n",
       "      (transformer): TransformerStack(\n",
       "        (blocks): ModuleList(\n",
       "          (0-29): 30 x UnifiedTransformerBlock(\n",
       "            (attn): MultiHeadAttention(\n",
       "              (layernorm_qkv): Sequential(\n",
       "                (0): LayerNorm((960,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=960, out_features=2880, bias=False)\n",
       "              )\n",
       "              (out_proj): Linear(in_features=960, out_features=960, bias=False)\n",
       "              (q_ln): LayerNorm((960,), eps=1e-05, elementwise_affine=True)\n",
       "              (k_ln): LayerNorm((960,), eps=1e-05, elementwise_affine=True)\n",
       "              (rotary): RotaryEmbedding()\n",
       "            )\n",
       "            (ffn): Sequential(\n",
       "              (0): LayerNorm((960,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=960, out_features=5120, bias=False)\n",
       "              (2): SwiGLU()\n",
       "              (3): Linear(in_features=2560, out_features=960, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((960,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (sequence_head): Sequential(\n",
       "        (0): Linear(in_features=960, out_features=960, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): LayerNorm((960,), eps=1e-05, elementwise_affine=True)\n",
       "        (3): Linear(in_features=960, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=960, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "    (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Dropout(p=0.1, inplace=False)\n",
       "    (9): Linear(in_features=256, out_features=356, bias=True)\n",
       "  )\n",
       "  (loss): CrossEntropyLoss()\n",
       "  (train_acc): MulticlassAccuracy()\n",
       "  (val_acc): MulticlassAccuracy()\n",
       "  (test_acc): MulticlassAccuracy()\n",
       "  (f1): MulticlassF1Score()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that TransformerStack now uses `MultiHeadAttention`\n",
    "# instead.\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = MultiLabelMLP(\n",
    "    embedder=model.embedder,\n",
    "    task=task,\n",
    "    num_classes=module.num_classes,\n",
    "    config=eval_config,\n",
    "    pad_id=tokenizer.pad_token_id,\n",
    "    eos_id=tokenizer.eos_token_id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = classifier.to(device, dtype=torch.bfloat16)\n",
    "batch = batch.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = classifier(batch)\n",
    "out.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
