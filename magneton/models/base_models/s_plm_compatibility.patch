diff --git a/model.py b/model.py
index 2d56b92..aba0155 100644
--- a/model.py
+++ b/model.py
@@ -6,7 +6,7 @@ from collections.abc import Sequence
 from transformers import EsmModel, T5Tokenizer, T5Model
 from transformers import BitsAndBytesConfig
 from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
-from utils import load_configs, get_dummy_logging
+from splm_utils import load_configs, get_dummy_logging
 import esm_adapterH
 import esm
 import numpy as np
@@ -81,7 +81,7 @@ def prepare_hf_esm_model(model_name, configs, logging):
         if configs.encoder.quantization_4_bit:
             if logging:
                 logging.info('make embedding parameters trainable because of 4 bit training')
-            
+
             for param in model.embeddings.word_embeddings.parameters():
                 param.requires_grad = True
 
@@ -109,7 +109,7 @@ def prepare_hf_esm_model(model_name, configs, logging):
     if configs.encoder.tune_embedding:
         if logging:
            logging.info('make embedding parameters trainable')
-        
+
         for param in model.embeddings.word_embeddings.parameters():
             param.requires_grad = True
 
@@ -123,7 +123,7 @@ def prepare_hf_esm_model(model_name, configs, logging):
 def prepare_esm_model(configs, logging=None):
     if logging:
         logging.info("use ESM model")
-    
+
     model_name = configs.encoder.model_name.split('/')[-1]
 
     # Create the model dynamically using module attributes
@@ -152,7 +152,7 @@ def prepare_esm_model(configs, logging=None):
             for idx in range(start_layer_idx, num_layers):
                 for layer_name in lora_targets:
                     target_modules.append(f"layers.{idx}.{layer_name}")
-        
+
         config = LoraConfig(
             r=configs.encoder.lora.r,
             lora_alpha=configs.encoder.lora.lora_alpha,
@@ -176,23 +176,23 @@ def prepare_esm_model(configs, logging=None):
             for param in model.emb_layer_norm_after.parameters():
                 param.requires_grad = True
     elif hasattr(configs.encoder,"prompt"):
-         
+
          if configs.encoder.prompt.enable:
             if not hasattr(configs.encoder.prompt,"num_tasks"):
                configs.encoder.prompt.num_tasks = 1
-            
+
             model.prefix_module = PrefixTuning(model, prompt_len=configs.encoder.prompt.prompt_len,
                                 prompt_layer_indices=configs.encoder.prompt.prompt_layer_indices,
                                 #num_tasks = configs.encoder.prompt.num_tasks
                                 )
             for param in model.prefix_module.parameters():
                 param.requires_grad = True
-        
-    
+
+
     if configs.encoder.tune_embedding:
         if logging:
            logging.info('make esm embedding parameters trainable')
-        
+
         for param in model.embed_tokens.parameters():
             param.requires_grad = True
 
@@ -202,7 +202,7 @@ def prepare_esm_model(configs, logging=None):
 def prepare_adapter_h_model(configs, logging=None):
     if logging:
        logging.info("use adapterH ESM model")
-    
+
     adapter_args = configs.encoder.adapter_h
     model_name = configs.encoder.model_name.split('/')[-1]
 
@@ -213,15 +213,15 @@ def prepare_adapter_h_model(configs, logging=None):
     # Freeze all layers
     for param in model.parameters():
         param.requires_grad = False
-    
+
     if configs.encoder.adapter_h.enable:
       if not isinstance(configs.encoder.adapter_h.freeze_adapter_layers, list):
         configs.encoder.adapter_h.freeze_adapter_layers = [configs.encoder.adapter_h.freeze_adapter_layers]
-    
+
     if configs.encoder.fine_tune.enable:
       if not isinstance(configs.encoder.fine_tune.freeze_adapter_layers, list):
         configs.encoder.fine_tune.freeze_adapter_layers = [configs.encoder.fine_tune.freeze_adapter_layers]
-    
+
     if configs.encoder.lora.enable:
         if logging:
            logging.info('enable LoRa on top of adapterH model')
@@ -236,7 +236,7 @@ def prepare_adapter_h_model(configs, logging=None):
             for idx in range(start_layer_idx, num_layers):
                 for layer_name in lora_targets:
                     target_modules.append(f"layers.{idx}.{layer_name}")
-        
+
         config = LoraConfig(
             r=configs.encoder.lora.r,
             lora_alpha=configs.encoder.lora.lora_alpha,
@@ -261,8 +261,8 @@ def prepare_adapter_h_model(configs, logging=None):
         if configs.encoder.fine_tune.last_layers_trainable != 0:
             for param in model.emb_layer_norm_after.parameters():
                 param.requires_grad = True
-    
-    
+
+
     # only freeze all the parameters once at the beginning. then open some layers later
     #only make adapterH trainable according to freeze_adapter_layers
     if configs.encoder.adapter_h.enable:
@@ -272,7 +272,7 @@ def prepare_adapter_h_model(configs, logging=None):
                 adapter_name = f"adapter_{adapter_idx}"
                 if adapter_name in name:
                     param.requires_grad = True
-    
+
     # only freeze all the parameters once at the beginning. then open some layers later,but because
     # of fine_tune, adapter layers might be tunable.
     #change on 1/15/2024 not need to use freeze_adapter_layers to control fine-tune part! use another parameter instead and must after setting of freeze_adapter_layers
@@ -289,14 +289,14 @@ def prepare_adapter_h_model(configs, logging=None):
       if configs.encoder.prompt.enable:
         if not hasattr(configs.encoder.prompt,"num_tasks"):
             configs.encoder.prompt.num_tasks = 1
-        
+
         model.prefix_module = PrefixTuning(model, prompt_len=configs.encoder.prompt.prompt_len,
                                 prompt_layer_indices=configs.encoder.prompt.prompt_layer_indices,
                                 #num_tasks = configs.encoder.prompt.num_tasks
                                 )
         for param in model.prefix_module.parameters():
             param.requires_grad = True
-    
+
     if configs.encoder.tune_embedding:
         for param in model.embed_tokens.parameters():
             param.requires_grad = True
@@ -371,32 +371,32 @@ class SequenceRepresentation(nn.Module):
         if hasattr(configs.encoder,"merge2ESM2"):
            if configs.encoder.merge2ESM2.enable:
               self.merge2ESM2 = True
-        
+
         if self.merge2ESM2:
               self.baseesm2, self.alphabet = prepare_esm_model(configs, logging)
-        
+
         if configs.encoder.adapter_h.enable:
             self.esm2, self.alphabet = prepare_adapter_h_model(configs, logging)
         else:
             self.esm2, self.alphabet = prepare_esm_model(configs, logging)
-        
+
         # self.device = device
         self.configs = configs
         #self.batch_converter = self.alphabet.get_batch_converter(truncation_seq_length=configs.encoder.max_len)
         self.batch_converter = self.alphabet.get_batch_converter()
         self.eval()
-    
+
     def forward(self, x,seq_only=False):
         with torch.no_grad():
             residue_representation = self.esm2(x, repr_layers=[self.esm2.num_layers], return_contacts=False)['representations'][
                                      self.esm2.num_layers]
-            #used in model pretrainning 
+            #used in model pretrainning
             if seq_only:
                 mask = ((x != self.alphabet.padding_idx) & (x != self.alphabet.cls_idx) & (
                          x != self.alphabet.eos_idx))
             else:
                 mask = (x != self.alphabet.padding_idx).to(x.device)  # use this in v2 training
-            
+
             denom = torch.sum(mask, -1, keepdim=True)
             if not self.merge2ESM2:
                 protein_representation = torch.sum(residue_representation * mask.unsqueeze(-1), dim=1) / denom  # remove padding
@@ -404,7 +404,7 @@ class SequenceRepresentation(nn.Module):
             else:
                 residue_representation_ESM2 = self.baseesm2(x,repr_layers=[self.baseesm2.num_layers],return_contacts=False)['representations'][
                                               self.baseesm2.num_layers]
-                
+
                 residue_representation_merged = (residue_representation+residue_representation_ESM2)/2
                 protein_representation_merged = torch.sum(residue_representation_merged * mask.unsqueeze(-1), dim=1) / denom  # remove padding
                 return protein_representation_merged,residue_representation_merged,mask
@@ -442,7 +442,7 @@ def prepare_configs_mergedESM2(configs):
            merged_configs.encoder.lora = configs.encoder.merge2ESM2.lora
         if hasattr(configs.encoder.merge2ESM2,"adapter_h"):
            merged_configs.encoder.adapter_h = configs.encoder.merge2ESM2.adapter_h
-        
+
         return merged_configs
 
 class Encoder_merge(nn.Module):
@@ -460,18 +460,18 @@ class Encoder_merge(nn.Module):
             #both merged ESM2 and ESM2 use esm_model
             self.baseesm2, self.alphabet = prepare_esm_model(merged_configs, logging)
             self.esm2, self.alphabet = prepare_esm_model(configs, logging)
-        
+
         self.head = nn.Linear(self.esm2.embed_dim, configs.encoder.num_classes)
         self.pooling_layer = nn.AdaptiveAvgPool1d(output_size=1)
         # self.device = device
         self.configs = configs
-    
+
     def forward(self, x):
         features1 = self.esm2(x['input_ids'],
                             repr_layers=[self.esm2.num_layers])['representations'][self.esm2.num_layers]
         features2 = self.baseesm2(x['input_ids'],
                             repr_layers=[self.baseesm2.num_layers])['representations'][self.baseesm2.num_layers]
-        
+
         features=(features1+features2)/2
         transposed_feature = features.transpose(1, 2)
         pooled_features = self.pooling_layer(transposed_feature).squeeze(2)
@@ -522,16 +522,16 @@ class EncoderSSPTM_merge(nn.Module):
             #both merged ESM2 and ESM2 use esm_model
             self.baseesm2, self.alphabet = prepare_esm_model(merged_configs, logging)
             self.esm2, self.alphabet = prepare_esm_model(configs, logging)
-        
+
         # extract the embedding size
         mlp_input_dim = self.esm2.embed_dim
-        
+
         mlp_hidden_dim = configs.encoder.mlp_hidden_dim
         mlp_layer_num = configs.encoder.mlp_layer_num
         hidden_dims = [mlp_hidden_dim] * (mlp_layer_num - 1)
         self.mlp = MultiLayerPerceptron(mlp_input_dim, hidden_dims + [configs.encoder.num_classes], batch_norm=False,
                                         dropout=configs.encoder.head_dropout)
-        
+
         # self.device = device
         self.configs = configs
 
@@ -540,8 +540,8 @@ class EncoderSSPTM_merge(nn.Module):
                             repr_layers=[self.esm2.num_layers])['representations'][self.esm2.num_layers]
         features2 = self.baseesm2(x['input_ids'],
                             repr_layers=[self.baseesm2.num_layers])['representations'][self.baseesm2.num_layers]
-        
-        c = self.mlp((features1[:, 1:-1, :]+features2[:,1:-1,:])/2) 
+
+        c = self.mlp((features1[:, 1:-1, :]+features2[:,1:-1,:])/2)
         #1:-1 is just remove start end or last padding. It is fine because we will have a mask tensor with only the effect resiue == 1
         return c
 
diff --git a/utils.py b/splm_utils.py
similarity index 100%
rename from utils.py
rename to splm_utils.py
