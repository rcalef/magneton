{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",ESMCBaseModel
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from magneton.config import DataConfig\n",
    "from magneton.data import MetaDataset, SubstructType, collate_meta_datasets\n",
    "from magneton.types import DataType\n",
    "\n",
    "from magneton.embedders.esmc_embedder import ESMCEmbedder, ESMCConfig, ESMC_300M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides an example of how to generate ESM-C embeddings using an existing protein dataset. Note that we need to specify both the location of the protein dataset directory as well as the path to the FASTA file containing the protein sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path(\"/n/holylfs06/LABS/mzitnik_lab/Lab/rcalef/datasets/magneton-data\")\n",
    "\n",
    "interpro_path = dataset_path / \"interpro_103.0\" / \"debug_datasets\" / \"val_sharded\"\n",
    "labels_path = dataset_path / \"interpro_103.0\" / \"selected_subset\"\n",
    "fasta_path = dataset_path / \"sequences\" / \"uniprot_sprot.fasta.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = DataConfig(\n",
    "    data_dir=interpro_path,\n",
    "    prefix=\"swissprot.with_ss.val\",\n",
    "    fasta_path=fasta_path,\n",
    "    labels_path=labels_path,\n",
    "    substruct_types=[SubstructType.DOMAIN],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.35s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3906"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prot_dataset = MetaDataset(\n",
    "    data_config=data_config,\n",
    "    want_datatypes=[DataType.SEQ, DataType.SUBSTRUCT],\n",
    "    load_fasta_in_mem=True,\n",
    ")\n",
    "len(prot_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(protein_ids=['A0A024FA41', 'A0A059J0G5', 'A0A068FIK2', 'A0A075B6I6'], seqs=['MPSTTQTTVQSIDSIDSIPTTIKRRQNDKTKTPKTKPVSKIPICPKNSSIPRLDQPSQHKFILLQSLLPITVHQLTTLVLSISRYDDYVHPFLLRLCVIIGYGYAFRFLLRREGLAIRTLGKKLGYLDGDHHPRDKVPRDSTRLNWSLPLTVGSRTVMCVLVAYDPSQQPINYLASLKWWAWLAVYLSLYPIILDFYYYCVHRAWHEVPCLWRFHRRHHTIKRPSILFTAYADSEQELFDIVGTPLLTFFTLKALHLPMDFYTWWICIQYIAYTEVMGHSGLRIYTTPPISCSWLLQRFGVELVIEDHDLHHRQGYRQARNYGKQTRIWDRLFGTCADRIETNPVNIQKGRRVMMHSINIPSLGN', 'MASQPPQPPSGQPDTQYEEYQSEVITETTNRPTPAADVYEITPTNDVMDDRYEHEHDDYESGAMYETVRTWSPQSRPELVRIASVFSRIDSHPDVAPTTEDGGQLNRRDTLAGVKIGDPVLDPTKPEFDFYKWARMFTHVMEKEGIKRNRTGVMFRNLTVLGSGSAVQYQDTFLSPFAAPFRPGELCGKGRNPEKVILHDFNGAIREGELLMVLGRPGSGCSTFLKAICGELHGLQKKKESIIHYNGVSQHTFKKELRGEAVYSAEDEHHFPHLTVGQTLEFAAAARTPSKRVLGLSRKDFSTHLARVMMSVFGLSHTYNTKVGDDYVRGVSGGERKRVSIAEIALSGAPICCWDNSTRGLDSATALEFTKALKIGSQVGGITQCLAIYQASQAIYDIFDKVIVLYEGRQIFFGPTRIAKQYFEEMGWYCPPRQTTADFLTSVTNPKERIAKEGYENRVPRTAVEFERYWKQSQNNKLLLANMDRFEAEYPPEEGHLEKLRETHGQAQAKHTASKSPYRISVPMQVKLCTVRAYQRLWGDKSSTIATNISQIMMALIIGSLFFDTPQTTDGFFAKGSVIFFAILLNGLMSITEINGLCKATEPIVPNAQRPIVVKHVNFAFYHAYSEALAGIVADIPIKFLLALVFNIIIYFLGGLERSAAKFFIFFLFTFITILTMSAIFRTLAAATKTIPQALALAGVMILALVIYTGFTLQPSYMHPWFKWILYINPIAYAYEALLVNEVHGNRYRCATPIPPYGSGKNFACAVAGAVPGEMSVSGDAWVESSYDYSYAHIWRNLGILLGFLAFFYFVYLMVSELNLSSASSAEFLVFRRGHLPKNFQGSKDEEAAAGGVMHPNDPARLPPTNTNGAAGETAPGGSTVAVIPPQKDIFTWRNVTYDITIKGEPRRLLDNISGWVRPGTLTALMGVSGAGKTTLLDALAQRTTMGVITGDMLVNGRPLDSSFQRKTGYVQQQDLHLETTTVREALRFSADLRQPKSVSRKEKYEYVEDVIKMLSMEDFSEAVVGNPGEGLNVEQRKLLTIGVELAAKPQLLLFLDEPTSGLDSQSSWSIVTFLRKLADNGQAVLSTIHQPSGILFEQFDRLLFLAKGGRTVYFGDIGKNSETLLNYFETHGAEPCGPSENPAEYMLNIVGAGPSGKSNIDWPVVWKESEESRHVQQELDRIQSETSKRNEGHGQSAEKEPGEFAMPFTSQLYCVTTRVFQQYWRTPSYIWGKLLLGLASALFIGFSFFLQNSSMAGLQNSLFSIFMLTTIFSSLVQQIMPRFVTQRDLFEVRERPSRAYSWKVFLLANIIVEIPYQILLGIIAWASLFYPTFGAHLSSERQGILLLYCVQFFIFASTFAQMIIAGLPDAETAGGIATTMFGLMVTFNGVLQKPNALPGFWRFMWRVSPITYTVGGLAATSLHSREVKCAQNELAIFDPPSGATCAQYLQKLVEAGAPGKLYNPMSTSQCQYCPLSSGDQFLGGSEIHWSDRWRNFGIGWAYIVFNIFATVALYYLIRVRKSSGRPNRIISVITYHLSQFGTYCRAFITGRKEKCPRKREQIGKIY', 'MEVGGGSEECCVKVAVHVRPLIGDEKVQGCKDCVTVIPGKPQVQIGTHSFTFDHVYGSTSSPSWMFEECIVPLVDGLFQGYNATVLAYGQTGSGKTYTMGTGFKGGSQTGIIPQVMNALFSKIENLKHQIEFQLHVSFIEILKEEVRDLLDPTFLNKSDTASANTGKVNVPGKPPIQIRESSDGVITLAGSTEVSVSTLKEMGACLEQGSLSRATGSTNMNNQSSRSHAIFTITLEQMRKLNPVSGDGNPNDSMSEEYLCAKLHLVDLAGSERAKRTGSDGMRFKEGVHINKGLLALGNVISALGDEKKRKEGVHVPYRDSKLTRLLQDSLGGNSRTVMIACISPADINAEETLNTLKYANRARNIQNKPVVNRDPMSNEILKMRQQLEYLQAELCARGGSGEVQVLNERIAWLEAANEDLCRELYEYRSRCTIVEQREMDAQDGSPCSVESDGLKRNLRSRESRDNQIVETMIGGDSREIEEGAAKEWEHMLLQNTMDKELHELNRQLEEKESEMKVFGGHTVALKQHFGKKIQELEEEKRAVQQERDRLLAEIENLSAGSEGQALKVHDIHAQKLKSLEAQIMDLKKKQENQVQLLKKKQKSDEAAKRLQDEIQYIKAQKVQLQHRIKQEAEQFRQWKASREKELLQLRKEGRRNEYERHKLQALNQRQKLVLQRKTEEAAMATKRLKELLEARKSAARDNLAIANGNGTNGKINEKGLQRWLDHELEVMVNVHEVRFEYEKQSQVRAALAEELAVLKQVDELDSKGPSPSRGKNGCARGSSLSPNARVARISSLEHMLGISSNSLVAMASQLSEAEERERAFTNRGRWNQLRSMGDAKNLLQYMFNSLGDSRYQLWEKGIEIREMKEQLKELVGLLRQSELQRKEVENELKLREQAVAIALATSATGNSPISLKHIDDDVKSSSSPMSVPAQKQLKYSPGIVNGPARESAAFIGQTRKMIPLGQLPMKNLVANGQAGNGKLWRWKRSHHQWLVQFKWKWQKPWRLSEWIRHSDETIIRARPRSQVLTYRV', 'MAWSSLLLTLLAHCTGSWAQSVLTQPPSVSGAPGQRVTISCTGSSSNIGAGYVVHWYQQLPGTAPKLLIYGNSNRPSGVPDQFSGSKSGTSASLAITGLQSEDEADYYCKAWDNSLNA'], substructures=[[LabeledSubstructure(ranges=[tensor([189, 335])], label=421, element_type=<SubstructType.DOMAIN: 'Domain'>)], [LabeledSubstructure(ranges=[tensor([525, 742]), tensor([1212, 1420])], label=551, element_type=<SubstructType.DOMAIN: 'Domain'>), LabeledSubstructure(ranges=[tensor([ 95, 175])], label=735, element_type=<SubstructType.DOMAIN: 'Domain'>), LabeledSubstructure(ranges=[tensor([167, 432]), tensor([ 891, 1134])], label=282, element_type=<SubstructType.DOMAIN: 'Domain'>)], [LabeledSubstructure(ranges=[tensor([  9, 374])], label=184, element_type=<SubstructType.DOMAIN: 'Domain'>)], [LabeledSubstructure(ranges=[tensor([ 20, 118])], label=431, element_type=<SubstructType.DOMAIN: 'Domain'>)]], structure_list=None)\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(\n",
    "    prot_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_meta_datasets,\n",
    ")\n",
    "\n",
    "# Batch is a list of tuples, each tuple is a protein sequence and Protein object.\n",
    "batch = next(iter(dataloader))\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'300m'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(ESMC_300M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_flash_attn = True\n",
    "\n",
    "esmc_config = ESMCConfig(\n",
    "    model_size=ESMC_300M,\n",
    "    weights_path=\"/n/home08/rcalef/storage/Lab/shared_model_weights/esmc-300m-2024-12\",\n",
    "    # Note that this is also the default, the hidden states from\n",
    "    # the final transformer layer.\n",
    "    rep_layer=29,\n",
    "    use_flash_attn=use_flash_attn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = ESMCEmbedder(config=esmc_config)\n",
    "if use_flash_attn:\n",
    "    embedder = embedder.to(device=device, dtype=torch.bfloat16)\n",
    "else:\n",
    "    embedder = embedder.to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now embed a single amino acid sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPSTTQTTVQSIDSIDSIPTTIKRRQNDKTKTPKTKPVSKIPICPKNSSIPRLDQPSQHKFILLQSLLPITVHQLTTLVLSISRYDDYVHPFLLRLCVIIGYGYAFRFLLRREGLAIRTLGKKLGYLDGDHHPRDKVPRDSTRLNWSLPLTVGSRTVMCVLVAYDPSQQPINYLASLKWWAWLAVYLSLYPIILDFYYYCVHRAWHEVPCLWRFHRRHHTIKRPSILFTAYADSEQELFDIVGTPLLTFFTLKALHLPMDFYTWWICIQYIAYTEVMGHSGLRIYTTPPISCSWLLQRFGVELVIEDHDLHHRQGYRQARNYGKQTRIWDRLFGTCADRIETNPVNIQKGRRVMMHSINIPSLGN\n",
      "torch.Size([365, 960])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-2.1240e-02,  5.1270e-02,  3.3447e-02,  ...,  2.3193e-02,\n",
       "          7.0801e-03, -1.3062e-02],\n",
       "        [-4.7852e-02,  5.1270e-02, -2.5269e-02,  ...,  4.7363e-02,\n",
       "          1.9043e-02,  2.2461e-02],\n",
       "        [-5.7617e-02, -1.8768e-03, -5.1758e-02,  ..., -2.8839e-03,\n",
       "         -6.9580e-03, -1.7334e-02],\n",
       "        ...,\n",
       "        [-3.2227e-02,  4.4678e-02, -8.6594e-04,  ...,  1.8555e-02,\n",
       "          1.3123e-02,  1.3184e-02],\n",
       "        [-3.8574e-02,  4.4189e-02,  2.2095e-02,  ...,  3.5156e-02,\n",
       "          3.8086e-02,  1.2144e-06],\n",
       "        [-2.6978e-02,  7.6660e-02, -2.2949e-02,  ...,  3.4180e-03,\n",
       "          4.2725e-03,  3.6011e-03]], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embed a single sequence\n",
    "print(batch.seqs[0])\n",
    "seq_embed = embedder.embed_single_protein(batch.seqs[0])\n",
    "\n",
    "print(seq_embed.shape)\n",
    "seq_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or embed a whole batch, returning a list of `torch.Tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sequences: 100%|█████████████████████████████████████████████████| 4/4 [00:00<00:00, 35.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([365, 960])\n",
      "torch.Size([1567, 960])\n",
      "torch.Size([1033, 960])\n",
      "torch.Size([118, 960])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-2.1240e-02,  5.1270e-02,  3.3447e-02,  ...,  2.3193e-02,\n",
       "          7.0801e-03, -1.3062e-02],\n",
       "        [-4.7852e-02,  5.1270e-02, -2.5269e-02,  ...,  4.7363e-02,\n",
       "          1.9043e-02,  2.2461e-02],\n",
       "        [-5.7617e-02, -1.8768e-03, -5.1758e-02,  ..., -2.8839e-03,\n",
       "         -6.9580e-03, -1.7334e-02],\n",
       "        ...,\n",
       "        [-3.2227e-02,  4.4678e-02, -8.6594e-04,  ...,  1.8555e-02,\n",
       "          1.3123e-02,  1.3184e-02],\n",
       "        [-3.8574e-02,  4.4189e-02,  2.2095e-02,  ...,  3.5156e-02,\n",
       "          3.8086e-02,  1.2144e-06],\n",
       "        [-2.6978e-02,  7.6660e-02, -2.2949e-02,  ...,  3.4180e-03,\n",
       "          4.2725e-03,  3.6011e-03]], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_embeds = embedder.embed_sequences(batch.seqs)\n",
    "print(len(batch_embeds))\n",
    "for embed in batch_embeds:\n",
    "    print(embed.shape)\n",
    "batch_embeds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also handle sequences longer than the max context length without any modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[365, 31340, 1033, 118]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.seqs[1] = batch.seqs[1]*20\n",
    "[len(seq) for seq in batch.seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sequences:   0%|                                                         | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sequences: 100%|█████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([365, 960])\n",
      "torch.Size([31340, 960])\n",
      "torch.Size([1033, 960])\n",
      "torch.Size([118, 960])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Note this is embedding sequences serially, batch-level tokenization has been pushed into the data loader\n",
    "batch_embeds = embedder.embed_sequences(batch.seqs)\n",
    "print(len(batch_embeds))\n",
    "for embed in batch_embeds:\n",
    "    print(embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
