{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from magneton.config import (\n",
    "    BaseModelConfig,\n",
    "    DataConfig,\n",
    "    ModelConfig,\n",
    "    PipelineConfig,\n",
    "    TrainingConfig,\n",
    ")\n",
    "from magneton.core_types import SubstructType\n",
    "from magneton.data import MagnetonDataModule\n",
    "from magneton.data.core import get_substructure_parser\n",
    "from magneton.models.substructure_classifier import SubstructureClassifier\n",
    "\n",
    "from magneton.utils import get_data_dir, get_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides an example of how to generate ESM-C embeddings using an existing protein dataset. Note that we need to specify both the location of the protein dataset directory as well as the path to the FASTA file containing the protein sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpro_path = get_data_dir() / \"interpro_103.0\"\n",
    "\n",
    "dataset_path = interpro_path / \"debug_subset\"\n",
    "labels_path = interpro_path / \"labels\" / \"selected_subset\"\n",
    "splits_path = interpro_path / \"dataset_splits\" / \"seq_splits.tsv\"\n",
    "fasta_path = get_data_dir() / \"sequences\" / \"uniprot_sprot.fasta.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = DataConfig(\n",
    "    data_dir=dataset_path,\n",
    "    fasta_path=fasta_path,\n",
    "    labels_path=labels_path,\n",
    "    splits=splits_path,\n",
    "    substruct_types=[SubstructType.DOMAIN, SubstructType.ACT_SITE],\n",
    "    batch_size=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = MagnetonDataModule(\n",
    "    data_config=data_config,\n",
    "    # model_type specifies which model-specific transforms to use,\n",
    "    # e.g. tokenization\n",
    "    model_type=\"esmc\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proteins: 100%|████████████████████████████████████| 1/1 [00:04<00:00,  4.56s/it]\n",
      "INFO:magneton.data.core.unified_dataset:split train: got 3010 proteins\n",
      "INFO:magneton.data.data_modules:remaining proteins after length filter: 2957 / 3010\n"
     ]
    }
   ],
   "source": [
    "loader = data_module.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESMCBatch(protein_ids=['A1AGH4', 'A0RFP3', 'A1ADB6', 'A1A8Z8'], lengths=[190, 215, 556, 250], seqs=None, substructures=[[LabeledSubstructure(ranges=[tensor([  7, 190])], label=379, element_type=<SubstructType.DOMAIN: 'Domain'>)], [LabeledSubstructure(ranges=[tensor([69, 85])], label=80, element_type=<SubstructType.ACT_SITE: 'Active_site'>), LabeledSubstructure(ranges=[tensor([130, 144])], label=81, element_type=<SubstructType.ACT_SITE: 'Active_site'>)], [LabeledSubstructure(ranges=[tensor([184, 388])], label=755, element_type=<SubstructType.DOMAIN: 'Domain'>)], [LabeledSubstructure(ranges=[tensor([ 8, 17])], label=3, element_type=<SubstructType.ACT_SITE: 'Active_site'>)]], structure_list=None, labels=None, tokenized_seq=tensor([[ 0, 20, 17,  ...,  1,  1,  1],\n",
       "        [ 0, 20, 15,  ...,  1,  1,  1],\n",
       "        [ 0, 20,  8,  ..., 21,  4,  2],\n",
       "        [ 0, 20,  5,  ...,  1,  1,  1]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_batch = next(iter(loader))\n",
    "example_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a batch of tokenized sequences, we'll set up the substructure classification model using ESM-C 300M as the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = ModelConfig(\n",
    "    frozen_base_model=True,\n",
    "    pooling_mechanism=\"mean\",\n",
    "    # Parameters for the substructure classification heads.\n",
    "    # 'embed' is shorthand for the base model's embedding\n",
    "    # dimensionality\n",
    "    model_params={\n",
    "        \"hidden_dims\": [\"embed\"],\n",
    "        \"dropout_rate\": 0.1,\n",
    "    },\n",
    ")\n",
    "\n",
    "train_config = TrainingConfig(\n",
    "    loss_strategy=\"standard\",\n",
    ")\n",
    "\n",
    "base_model_config = BaseModelConfig(\n",
    "    model=\"esmc\",\n",
    "    model_params={\n",
    "        \"model_size\": \"300m\",\n",
    "        \"weights_path\": get_model_dir() / \"esmc-300m-2024-12\",\n",
    "        \"rep_layer\": 29,\n",
    "        \"use_flash_attn\": False,\n",
    "    }\n",
    ")\n",
    "\n",
    "config = PipelineConfig(\n",
    "    training=train_config,\n",
    "    model=model_config,\n",
    "    base_model=base_model_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:magneton.models.substructure_classifier:head model: ModuleDict(\n",
      "  (Active_site): Sequential(\n",
      "    (0): Linear(in_features=960, out_features=960, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=960, out_features=82, bias=True)\n",
      "  )\n",
      "  (Domain): Sequential(\n",
      "    (0): Linear(in_features=960, out_features=960, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=960, out_features=917, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "substruct_parser = get_substructure_parser(data_config)\n",
    "\n",
    "model = SubstructureClassifier(\n",
    "    config=config,\n",
    "    num_classes=substruct_parser.num_labels()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now get substructure embeddings, which are returned as a dict where the key is the type of substructure and the value is the tensor of embeddings for all substructures of that type within the batch. For example, if the batch contained two proteins with 3 domains each, rows 0-2 would contain the embeddings of the domains in the first protein and rows 3-5 would contain the embeddings of the domains in the second protein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "example_batch = example_batch.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<SubstructType.DOMAIN: 'Domain'>: tensor([[ 0.0269, -0.0068, -0.0080,  ...,  0.0192,  0.0055,  0.0119],\n",
       "         [ 0.0284,  0.0014, -0.0060,  ...,  0.0221,  0.0089,  0.0020]],\n",
       "        device='cuda:0'),\n",
       " <SubstructType.ACT_SITE: 'Active_site'>: tensor([[-0.0140,  0.0091,  0.0111, -0.0251, -0.0091, -0.0035, -0.0112,  0.0125,\n",
       "          -0.0079,  0.0201,  0.0069, -0.0143, -0.0636,  0.0019,  0.0099,  0.0214,\n",
       "          -0.0027,  0.0070,  0.0019,  0.0533,  0.0232, -0.0098, -0.0416,  0.0333,\n",
       "           0.0015,  0.0190, -0.0105,  0.0167,  0.0181, -0.0032,  0.0259,  0.0222,\n",
       "          -0.0283,  0.0168, -0.0105,  0.0007, -0.0137, -0.0252,  0.0022, -0.0214,\n",
       "          -0.0005, -0.0048, -0.0305, -0.0022,  0.0016, -0.0065, -0.0163, -0.0094,\n",
       "          -0.0002,  0.0033, -0.0212, -0.0178, -0.0090, -0.0198, -0.0196, -0.0311,\n",
       "          -0.0056,  0.0115,  0.0082,  0.0012, -0.0065,  0.0207,  0.0277,  0.0040,\n",
       "          -0.0102, -0.0328,  0.0147, -0.0080,  0.0240,  0.0485, -0.0223, -0.0033,\n",
       "           0.0112, -0.0237,  0.0098,  0.0289,  0.0132,  0.0044, -0.0229,  0.0132,\n",
       "           0.0277,  0.0153],\n",
       "         [-0.0089,  0.0061,  0.0160, -0.0125, -0.0135, -0.0041, -0.0012,  0.0169,\n",
       "          -0.0160,  0.0183,  0.0024, -0.0140, -0.0541, -0.0077,  0.0142,  0.0275,\n",
       "          -0.0027,  0.0120,  0.0041,  0.0554,  0.0159, -0.0107, -0.0376,  0.0267,\n",
       "          -0.0018,  0.0257, -0.0191,  0.0217,  0.0066, -0.0053,  0.0216,  0.0209,\n",
       "          -0.0318,  0.0162, -0.0165,  0.0013, -0.0073, -0.0388,  0.0033, -0.0151,\n",
       "          -0.0019, -0.0059, -0.0309, -0.0004, -0.0076, -0.0072, -0.0161, -0.0023,\n",
       "           0.0011,  0.0134, -0.0138, -0.0165, -0.0068, -0.0252, -0.0229, -0.0356,\n",
       "           0.0043,  0.0003,  0.0061,  0.0002, -0.0102,  0.0290,  0.0269,  0.0022,\n",
       "          -0.0007, -0.0290,  0.0218, -0.0136,  0.0299,  0.0473, -0.0128, -0.0031,\n",
       "           0.0118, -0.0219,  0.0190,  0.0269,  0.0200,  0.0035, -0.0082,  0.0191,\n",
       "           0.0239,  0.0221],\n",
       "         [-0.0134,  0.0013,  0.0124, -0.0149, -0.0041, -0.0136, -0.0202,  0.0130,\n",
       "          -0.0086,  0.0064, -0.0036, -0.0221, -0.0534,  0.0027,  0.0205,  0.0169,\n",
       "           0.0038,  0.0114,  0.0131,  0.0549,  0.0251, -0.0136, -0.0429,  0.0225,\n",
       "          -0.0066,  0.0204, -0.0066,  0.0214,  0.0108, -0.0139,  0.0348,  0.0211,\n",
       "          -0.0334,  0.0125, -0.0131, -0.0018, -0.0189, -0.0297,  0.0059, -0.0136,\n",
       "           0.0045, -0.0016, -0.0269, -0.0032, -0.0034, -0.0119, -0.0141, -0.0105,\n",
       "           0.0032,  0.0023, -0.0131, -0.0131, -0.0086, -0.0191, -0.0183, -0.0369,\n",
       "          -0.0059,  0.0024, -0.0001,  0.0101, -0.0061,  0.0248,  0.0306, -0.0059,\n",
       "          -0.0159, -0.0387,  0.0160, -0.0029,  0.0229,  0.0450, -0.0130, -0.0015,\n",
       "           0.0146, -0.0214,  0.0145,  0.0219,  0.0035,  0.0007, -0.0157,  0.0126,\n",
       "           0.0239,  0.0249]], device='cuda:0')}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    substruct_embeds = model(example_batch)\n",
    "substruct_embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want residue-level or protein-level embeddings from the base model (e.g. for downstream evaluation tasks after substructure-tuning), we can directly call the base model.\n",
    "\n",
    "Note that the method for computing protein-level embeddings varies across base models (e.g. mean pooling vs CLS token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 557, 960])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    residue_embeds = model.base_model.embed_batch(example_batch)\n",
    "residue_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 960])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    protein_embeds = model.base_model.embed_batch(example_batch, protein_level=True)\n",
    "protein_embeds.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
