{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from magneton.config import DataConfig\n",
    "from magneton.core_types import SubstructType\n",
    "from magneton.data import MagnetonDataModule\n",
    "from magneton.models.base_models.esmc import ESMCBaseModel, ESMCConfig, ESMC_300M\n",
    "from magneton.utils import get_data_dir, get_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = get_data_dir()\n",
    "model_path = get_model_dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides an example of how to generate ESM-C embeddings using an existing protein dataset. Note that we need to specify both the location of the protein dataset directory as well as the path to the FASTA file containing the protein sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpro_path = (\n",
    "    data_path /\n",
    "    \"interpro_103.0\"\n",
    ")\n",
    "\n",
    "label_path = (\n",
    "    interpro_path /\n",
    "    \"labels\" /\n",
    "    \"selected_subset\"\n",
    ")\n",
    "\n",
    "dataset_path = interpro_path / \"debug_subset\"\n",
    "fasta_path = data_path / \"sequences\" / \"uniprot_sprot.fasta.gz\"\n",
    "\n",
    "\n",
    "data_config = DataConfig(\n",
    "    data_dir=dataset_path,\n",
    "    prefix=\"swissprot.with_ss.train\",\n",
    "    fasta_path=fasta_path,\n",
    "    labels_path=label_path,\n",
    "    substruct_types=[SubstructType.DOMAIN],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = MagnetonDataModule(\n",
    "    data_config=data_config,\n",
    "    model_type=\"esmc\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████| 1/1 [00:13<00:00, 13.30s/it]\n"
     ]
    }
   ],
   "source": [
    "loader = data_module.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESMCBatch(protein_ids=['A1ABS6', 'A1AWD0', 'A0KU61', 'A0JXU0', 'A1AU61', 'A0BD73', 'A0ALA8', 'A1ADB6', 'A0A2H4HHY6', 'A0AEM3', 'A0AFC3', 'A0A455LLX4', 'A0T0M9', 'A0RJ81', 'A0KYA2', 'A0A0H3NBY9', 'A0RV25', 'A0KEH8', 'A0B9K1', 'A0RCM7', 'A0T0L8', 'A0QL16', 'A0QSG3', 'A1JLK6', 'A0Q3I1', 'A0R1W8', 'A0T0H8', 'A0KZ22', 'A0PW28', 'A0A0H2URG7', 'A0RPF9', 'A1JNH0'], seqs=None, substructures=None, structure_list=None, labels=None, tokenized_seq=tensor([[ 0, 20, 17,  ...,  1,  1,  1],\n",
       "        [ 0, 20,  5,  ...,  1,  1,  1],\n",
       "        [ 0, 20, 21,  ...,  1,  1,  1],\n",
       "        ...,\n",
       "        [ 0, 20, 11,  ...,  1,  1,  1],\n",
       "        [ 0, 20, 12,  ...,  1,  1,  1],\n",
       "        [ 0, 20, 20,  ...,  1,  1,  1]]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch = next(it)\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_flash_attn = True\n",
    "\n",
    "esmc_config = ESMCConfig(\n",
    "    model_size=ESMC_300M,\n",
    "    weights_path=model_path / \"esmc-300m-2024-12\",\n",
    "    # Note that this is also the default, the hidden states from\n",
    "    # the final transformer layer.\n",
    "    rep_layer=29,\n",
    "    use_flash_attn=use_flash_attn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = ESMCBaseModel(config=esmc_config)\n",
    "if use_flash_attn:\n",
    "    embedder = embedder.to(device=device, dtype=torch.bfloat16)\n",
    "else:\n",
    "    embedder = embedder.to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now embed a single amino acid sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPSTTQTTVQSIDSIDSIPTTIKRRQNDKTKTPKTKPVSKIPICPKNSSIPRLDQPSQHKFILLQSLLPITVHQLTTLVLSISRYDDYVHPFLLRLCVIIGYGYAFRFLLRREGLAIRTLGKKLGYLDGDHHPRDKVPRDSTRLNWSLPLTVGSRTVMCVLVAYDPSQQPINYLASLKWWAWLAVYLSLYPIILDFYYYCVHRAWHEVPCLWRFHRRHHTIKRPSILFTAYADSEQELFDIVGTPLLTFFTLKALHLPMDFYTWWICIQYIAYTEVMGHSGLRIYTTPPISCSWLLQRFGVELVIEDHDLHHRQGYRQARNYGKQTRIWDRLFGTCADRIETNPVNIQKGRRVMMHSINIPSLGN\n",
      "torch.Size([365, 960])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-2.1240e-02,  5.1270e-02,  3.3447e-02,  ...,  2.3193e-02,\n",
       "          7.0801e-03, -1.3062e-02],\n",
       "        [-4.7852e-02,  5.1270e-02, -2.5269e-02,  ...,  4.7363e-02,\n",
       "          1.9043e-02,  2.2461e-02],\n",
       "        [-5.7617e-02, -1.8768e-03, -5.1758e-02,  ..., -2.8839e-03,\n",
       "         -6.9580e-03, -1.7334e-02],\n",
       "        ...,\n",
       "        [-3.2227e-02,  4.4678e-02, -8.6594e-04,  ...,  1.8555e-02,\n",
       "          1.3123e-02,  1.3184e-02],\n",
       "        [-3.8574e-02,  4.4189e-02,  2.2095e-02,  ...,  3.5156e-02,\n",
       "          3.8086e-02,  1.2144e-06],\n",
       "        [-2.6978e-02,  7.6660e-02, -2.2949e-02,  ...,  3.4180e-03,\n",
       "          4.2725e-03,  3.6011e-03]], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embed a single sequence\n",
    "print(batch.seqs[0])\n",
    "seq_embed = embedder.embed_single_protein(batch.seqs[0])\n",
    "\n",
    "print(seq_embed.shape)\n",
    "seq_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or embed a whole batch, returning a list of `torch.Tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sequences: 100%|█████████████████████████████████████████████████| 4/4 [00:00<00:00, 35.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([365, 960])\n",
      "torch.Size([1567, 960])\n",
      "torch.Size([1033, 960])\n",
      "torch.Size([118, 960])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-2.1240e-02,  5.1270e-02,  3.3447e-02,  ...,  2.3193e-02,\n",
       "          7.0801e-03, -1.3062e-02],\n",
       "        [-4.7852e-02,  5.1270e-02, -2.5269e-02,  ...,  4.7363e-02,\n",
       "          1.9043e-02,  2.2461e-02],\n",
       "        [-5.7617e-02, -1.8768e-03, -5.1758e-02,  ..., -2.8839e-03,\n",
       "         -6.9580e-03, -1.7334e-02],\n",
       "        ...,\n",
       "        [-3.2227e-02,  4.4678e-02, -8.6594e-04,  ...,  1.8555e-02,\n",
       "          1.3123e-02,  1.3184e-02],\n",
       "        [-3.8574e-02,  4.4189e-02,  2.2095e-02,  ...,  3.5156e-02,\n",
       "          3.8086e-02,  1.2144e-06],\n",
       "        [-2.6978e-02,  7.6660e-02, -2.2949e-02,  ...,  3.4180e-03,\n",
       "          4.2725e-03,  3.6011e-03]], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_embeds = embedder.embed_sequences(batch.seqs)\n",
    "print(len(batch_embeds))\n",
    "for embed in batch_embeds:\n",
    "    print(embed.shape)\n",
    "batch_embeds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also handle sequences longer than the max context length without any modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[365, 31340, 1033, 118]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.seqs[1] = batch.seqs[1]*20\n",
    "[len(seq) for seq in batch.seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sequences:   0%|                                                         | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sequences: 100%|█████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([365, 960])\n",
      "torch.Size([31340, 960])\n",
      "torch.Size([1033, 960])\n",
      "torch.Size([118, 960])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Note this is embedding sequences serially, batch-level tokenization has been pushed into the data loader\n",
    "batch_embeds = embedder.embed_sequences(batch.seqs)\n",
    "print(len(batch_embeds))\n",
    "for embed in batch_embeds:\n",
    "    print(embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
